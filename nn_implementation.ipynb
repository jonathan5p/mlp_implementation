{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcial 1\n",
    "Para el parcial 1 se desea crear un clasificador de partículas que arroje 1 para la partícula 1 y 0 para la partícula 2.\n",
    "\n",
    "El primer paso es importar las librerias necesarias para solucionar este problema: \n",
    "* Pandas: Librería utilziada para el manejo de bases de datos y extracción de las caracteristicas de cada columna(media,varianza,maximos,minimos).\n",
    "* numpy: Librería encargada del manejo de operaciones entre vectores y matrices.\n",
    "* matplotlib: Librería utilizada para realizar analisís sobre graficas. \n",
    "* metrics: Módulo de la librería scikit-learn, este modulo permite hallar diversas medidas de evaluación de modelos \n",
    "* preprocessing: Módulo que permite realizar diferentes preprocesamientos a los datos\n",
    "* SimpleImputer: Función que facilita el llenado de espacios en blanco, ceros o Nans en la base de datos de entrenamiento\n",
    "* decomposition: Módulo utilizado para realizar el proceso de analisís de componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#importar librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as mes\n",
    "np.random.seed(3)\n",
    "minn=0\n",
    "maxn=0\n",
    "std=0\n",
    "mean=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar\n",
    "<div class=text-justify>\n",
    "La función normalizar permite normalizar una matriz X dada por parametro utilizando las variables minn y maxn declaradas en la celda anterior. Dado que se desea normalizar tanto el set de entrenamiento como el de test en función de los datos de entramiento, la variable minn es un vector que contiene los minimos de cada variable(columna) en el set de entrenamiento, y maxn es un vector que contiene los maximos de cada variable(columna) en el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def normalizar(X):\n",
    "    global minn,maxn\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    X -- Matriz de datos que se desea normalizar\n",
    "    minn -- Vector con los minimos de las columnas del set de datos de entrenamiento\n",
    "    maxn -- Vector con los maximos de las columnas del set de datos de entrenamiento\n",
    "    \n",
    "    Salidas:\n",
    "    Xnorm -- Matriz X normalizada con el valor minimo y maximo de cada columna de la matriz de entrenamiento \n",
    "    \"\"\"\n",
    "    resta=maxn-minn\n",
    "    Xnorm=(X-minn)/resta\n",
    "    return Xnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarizar\n",
    "<div class=text-justify>\n",
    "La función estandarizar permite estandarizar una matriz X dada por parametro utilizando la media y la varizanda de la matriz que contiene los ejemplos utilizados durante el entrenamiento de la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def estandarizar(X):\n",
    "    global std,mean\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    X -- Matriz de datos que se desea estandarizar\n",
    "    mean -- Vector con las medias de las columnas del set de datos de entrenamiento\n",
    "    std -- Vector con las desviaciones estandar de las columnas del set de datos de entrenamiento\n",
    "    \n",
    "    Salidas:\n",
    "    Xnorm -- Matriz X estandarizada, es decir, las columnas de la matriz Xnorm tienen media 0 y varianza 1 \n",
    "    \"\"\"\n",
    "    \n",
    "    Xnorm=(X-mean)/np.power(std,2)\n",
    "    return Xnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing \n",
    "\n",
    "Esta función se encarga de preprocesar los datos de entrada utilizando el método introducido en el parámetro de entrada type. La función cuenta con los siguientes tipos de preprocesamiento: \n",
    "\n",
    "+ pca: Analisís de componentes principales de los datos de entrenamiento. Para mas información <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA>\n",
    "+ normalizar: Escalado de las columnas de las matriz de entrenamiento con el fin de dejar todos los valores de la matriz en una escala entre 0 y 1.\n",
    "+ estandarizar: Escalado de las columnas de la matriz de entrenamiento con el fin de dejar todas las variables de la matriz con media 0 y varianza unitaria. \n",
    "+ robust: Escalado de los descriptores de la matriz de entrenamiento utilizando métodos estadisticos que son robustos frente a datos ruidosos ó \"outliers\". Para mas información <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocesing(type='normalizar',whiten='False'):\n",
    "    global minn,maxn,std,mean\n",
    "    \n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    type -- Tipo de presocesamiento que se le desea realizar a la base de datos\n",
    "    whiten -- En caso de que el tipo de procesamiento seleccionado sea 'pca', indica si se debe realizar blanqueamiento a los datos \n",
    "    \n",
    "    Salidas:\n",
    "    Xtrain -- Matriz que contiene los datos de entrenamiento ya preprocesados\n",
    "    Ytrain -- Etiquetas asignadas a cada uno de los ejemplos del set de entrenamiento\n",
    "    Xcv -- Matriz que contiene los datos de validación ya preprocesados\n",
    "    Ycv -- Etiquetas asignadas a cada uno de los ejemplos del set de validación \n",
    "    Xtest -- Matriz que contiene los datos de prueba\n",
    "    Ytest -- Etiquetas asignadas a cada ejemplo en el set de prueba\n",
    "    XtestT -- Matriz de datos de prueba que se utilizarán para calificar el modelo hallado\n",
    "    pca -- En caso de que el tipo de preprocesamiento seleccionado sea 'pca', devuelve el objeto que contiene \n",
    "           todos los parámetros con los que se realizo el analisí de componentes principales\n",
    "    \"\"\"\n",
    "    \n",
    "    imp = SimpleImputer(missing_values=0.0, strategy='mean')\n",
    "    XtrainT=pd.read_csv('xtrainCSV.csv',sep=\";\",header=None)\n",
    "    XtrainT=pd.DataFrame(imp.fit_transform(XtrainT))\n",
    "    \n",
    "    YtrainT=pd.read_csv('ytrain.txt',header=None)\n",
    "    XtestT=pd.read_csv('xtestCSV.csv',sep=\";\",header=None)\n",
    "    nt= int(0.70*XtrainT.shape[0]) #numero de ejemplos de entreno\n",
    "    ncv=int(0.15*XtrainT.shape[0]) #numero de ejemplos para el set de validación \n",
    "    \n",
    "    Xtrain=XtrainT.sample(n=nt) #Datos de entrenamiento\n",
    "    Ytrain=YtrainT.iloc[Xtrain.index,:]\n",
    "\n",
    "    Xmed=XtrainT.drop(XtrainT.index[Xtrain.index])\n",
    "    Ymed=YtrainT.drop(YtrainT.index[Xtrain.index])\n",
    "\n",
    "    Xcv=Xmed.sample(n=ncv)\n",
    "    Ycv=Ymed.loc[Xcv.index,:]\n",
    "    \n",
    "    Xmed=Xmed.drop(XtrainT.index[Xcv.index])\n",
    "    Ymed=Ymed.drop(YtrainT.index[Xcv.index])\n",
    "\n",
    "    Xtest=Xmed\n",
    "    Ytest=Ymed\n",
    "    \n",
    "    pca=PCA(n_components=25,whiten=whiten)\n",
    "    if type=='pca':\n",
    "        pca.fit(Xtrain)\n",
    "        Xtrain=pd.DataFrame(pca.transform(Xtrain))\n",
    "        Xcv=pd.DataFrame(pca.transform(Xcv))\n",
    "        Xtest=pd.DataFrame(pca.transform(Xtest))\n",
    "        index1=pd.Index(range(Xtrain.shape[0]))\n",
    "        Ytrain=Ytrain.set_index(index1)\n",
    "        index2=pd.Index(range(Xcv.shape[0]))\n",
    "        Ycv=Ycv.set_index(index2)\n",
    "        index3=pd.Index(range(Xtest.shape[0]))\n",
    "        Ytest=Ytest.set_index(index3)\n",
    "        \n",
    "        XtestT = pd.DataFrame(pca.transform(XtestT))\n",
    "        \n",
    "    elif type=='normalizar':\n",
    "        minn=Xtrain.min(axis=0)\n",
    "        maxn=Xtrain.max(axis=0)\n",
    "        Xtrain=normalizar(Xtrain)\n",
    "        Xcv=normalizar(Xcv)\n",
    "        Xtest=normalizar(Xtest)\n",
    "        \n",
    "        XtestT=normalizar(XtestT)\n",
    "        \n",
    "    elif type=='estandarizar':\n",
    "        std=Xtrain.std(axis=0)\n",
    "        mean=Xtrain.mean(axis=0)\n",
    "            \n",
    "        Xtrain=estandarizar(Xtrain)\n",
    "        Xcv=estandarizar(Xcv)\n",
    "        Xtest=estandarizar(Xtest)\n",
    "        \n",
    "        XtestT=estandarizar(XtestT)\n",
    "    \n",
    "    elif type=='robust':\n",
    "        transformer=RobustScaler().fit(Xtrain)\n",
    "        Xtrain=pd.DataFrame(transformer.transform(Xtrain))\n",
    "        Xcv=pd.DataFrame(transformer.transform(Xcv))\n",
    "        Xtest=pd.DataFrame(transformer.transform(Xtest))\n",
    "        XtestT=pd.DataFrame(transformer.transform(XtestT))\n",
    "        index1=pd.Index(range(Xtrain.shape[0]))\n",
    "        Ytrain=Ytrain.set_index(index1)\n",
    "        index2=pd.Index(range(Xcv.shape[0]))\n",
    "        Ycv=Ycv.set_index(index2)\n",
    "        index3=pd.Index(range(Xtest.shape[0]))\n",
    "        Ytest=Ytest.set_index(index3)\n",
    "\n",
    "    return Xtrain,Ytrain,Xcv,Ycv,Xtest,Ytest,XtestT,pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones que componen la red neuronal: \n",
    "<div class=text-justify>\n",
    "    \n",
    "A continuación, se presentan las funciones utilizadas para entrenar la red neuronal que creara el modelo de clasificación. Para esta implementación de una red neuronal, se agregaron nuevas variables de memoria llamadas ***caches*** que permiten guardar ciertos calculos realizados durante el proceso de forward-propagation que son de utilidad al momento de realizar back-prop. El uso de variables de memoria o ***caches***, optimiza el proceso de back-propagation al evitar recalcular variables ya utilizadas en forward-propagation, esto se ve reflejado en la eficiencia del algoritmo en cuanto a lo que a tiempo de procesamiento se refiere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation_function: \n",
    "Esta función se encarga de retornar la activación de una  neurona dada su función de ***pre-activación=Z*** y la función de activación a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def activation_function(Z,activation):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    Z -- Matriz con las salidas pre-activación de la capa actual\n",
    "    activation -- función de activación seleccionada \n",
    "    \n",
    "    Salidas:\n",
    "    A -- Matriz con las activaciones de la capa actual\n",
    "    cache -- Datos almacenados para mejorar la eficiencia de backpropagation \n",
    "    \"\"\"\n",
    "    cache=Z\n",
    "    if activation=='sigmoid':\n",
    "        A= 1/(1+np.exp(-Z))\n",
    "    elif activation=='relu':\n",
    "        A=(Z>0)*Z\n",
    "    elif activation=='tanh':\n",
    "        Z=(3/2)*Z\n",
    "        A = 1.7159*(np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    return (A,cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters_deep: \n",
    "\n",
    "Función encargada de inicializar los pesos ***W*** y biases ***b*** de cada neurona en cada capa de la red.\n",
    "Los pesos se inicializan de manera aleatoria con valores entre 0 y 1, mientras que los biases se inicializan en zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    layer_dims -- arreglo que contiene las dimensiones de cada capa de la red neuronal\n",
    "    \n",
    "    Salidas:\n",
    "    parameters -- Diccionario de python que contiene los parametros de cada capa \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matriz de pesos de la capa l con dimensiones (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vector bias de la capa l con dimensiones (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # Numero de capas en la red\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters_adam: \n",
    "\n",
    "Proceso de inicialización del vector ***v*** que contiene el promedio móvil de los gradientes de la red, o en otras palabras, el primer momento. Así mismo, se inicializa el vector ***s*** que contiene el promedio móvil de los gradientes cuadrados de lared, o en otras palabras, el segundo momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    \n",
    "    parameters -- Diccionario de python que contiene los pesos y biases de todas las capas de la red.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Salidas: \n",
    "    v -- Diccionario que contiene la media movil exponencial de los gradientes de la red\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- Diccionario que contiene la media movil exponencial de los gradientes al cuadrado de la red.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_forward:\n",
    "\n",
    "Dadas las activaciones de las neuronas en la capa anterior, los pesos ***W*** y los biases ***b*** de la capa actual, la función ***linear_forward*** calcula las pre-activaciones de la capa actual (***Z***)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    A -- Activaciones de la capa anterior\n",
    "    W -- Matriz de pesos de la capa actual\n",
    "    b -- Vector bias de la capa actual\n",
    "\n",
    "    Salidas:\n",
    "    Z -- función de pre-activación (f(Z)=A) \n",
    "    cache -- Tupla de python que almacena los valores de \"A\", \"W\" y \"b\" ; Guardados con el fin de hacer backpropagation más eficiente\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_activation_forward: \n",
    "Utilizando la función ***linear_forward*** esta función es capaz de calcular las activaciones de las neuronas en la capa actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    A_prev -- Activaciones de la capa anterior\n",
    "    W -- Matriz de pesos de la capa actual\n",
    "    b -- Vector bias de la capa actual\n",
    "    activation -- función de activación utilizada en esta capa\n",
    "\n",
    "    Salidas:\n",
    "    A -- Activación de la capa actual  \n",
    "    cache -- Tupla de python que almacena \"linear_cache\" y \"activation_cache\";\n",
    "             alamacenadas para hacer backpropagation más eficiente\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = activation_function(Z,'sigmoid')\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = activation_function(Z,'relu')\n",
    "    elif activation == \"tanh\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = activation_function(Z,'tanh')\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-model-forward:\n",
    "\n",
    "L-model-forward utiliza la función ***linear_activation_forward*** para calcular las activaciones de todas las neuronas en todas las capas de la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters,keep_prob,act_function1='relu',act_function2='sigmoid'):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    X -- Datos de entrada\n",
    "    parameters -- parametros de la red (inicializados de manera aleatoria)\n",
    "    keep_prob -- probabilidad de dejar una neurona viva (utilizado en Dropout)\n",
    "    \n",
    "    Salidas:\n",
    "    AL -- Activación de la capa de salida\n",
    "    caches -- Lista de caches que contiene:\n",
    "                cada cache almacenado en linear_activation_forward() para cada capa de la red \n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # Numero de capas de la red\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        if l!=1:\n",
    "            D = np.random.rand(A_prev.shape[0],A_prev.shape[1])                               \n",
    "            D = D<=keep_prob                                         \n",
    "            A_prev = A_prev*D                                       \n",
    "            A_prev = A_prev/keep_prob\n",
    "        A, cache = linear_activation_forward(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],act_function1)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A,parameters['W' + str(L)],parameters['b' + str(L)],act_function2)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stretch_vector: \n",
    "Función auxiliar utilizada por ***compute_cost*** para poder calcular el valor de la regularización L2. ***stretch_vector*** recibe todos los parametros de la red (***W***,***b***) y devuelve un vector con todos los pesos y un vector con todos los bias de la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def stretch_vector(parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parámetros de entrada: \n",
    "    parameters -- parametros de la red\n",
    "    \n",
    "    Salidas: \n",
    "    \n",
    "    parameters_vectorW -- Arreglo de una dimensión con todos los pesos de la red  \n",
    "    parameters_vectorB -- Arreglo de una dimensión con todos los biases de la red\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    parameters_vectorW=np.array(np.squeeze(parameters['W'+str(L)]),ndmin=2)\n",
    "    parameters_vectorb=np.array(np.squeeze(parameters['b'+str(L)]),ndmin=2)\n",
    "    for l in range(L-1):\n",
    "        parameters_vectorW=np.concatenate((parameters_vectorW,np.array(np.squeeze(parameters['W'+str(l+1)]),ndmin=2)),axis=None)\n",
    "        parameters_vectorb=np.concatenate((parameters_vectorb,np.array(np.squeeze(parameters['b'+str(l+1)]),ndmin=2)),axis=None)\n",
    "    return (parameters_vectorW,parameters_vectorb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_cost:\n",
    "Cálculo de la función de costo utilizada, en este caso se utilizó la función de entropía cruzada más el termino de regularización L2. Donde el termino de regularización es la suma sobre todas las capas de cada uno de los pesos de la red.\n",
    "\n",
    "$$ J(X,Y,w)= \n",
    "-\\sum_{i=1}^{n}y_{i}Log(\\sigma(X,w))+(1-y_{i})Log(1-\\sigma(X,w))+\\frac{\\lambda}{2m}\\sum_{i=1}^{L}\\sum_{j=1}^{m}\\sum_{k=1}^{n}W_{j,k}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, lambd,parameters):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    AL -- Activaciones de la capa de salida\n",
    "    Y -- Vector de entrada con la clasificación real de cada ejemplo utilizado en el entrenamiento\n",
    "    \n",
    "    Salidas:\n",
    "    cost -- Valor de la función de costo para el modelo actual, para este caso la función de costo es la función de entropía cruzada\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    parameters_vectorW,parameters_vectorb=stretch_vector(parameters)\n",
    "    regularization_term=np.sum(np.square(parameters_vectorW))+np.sum(np.square(parameters_vectorb))\n",
    "    AL[AL==1]=0.9999999999999\n",
    "    AL[AL==0]=0.0000000000001\n",
    "    cost = (-1/m)*(np.dot(Y,np.transpose(np.log(AL)))+np.dot(np.ones((Y.shape))-Y,np.transpose(np.log(np.ones((Y.shape))-AL))))+(lambd/(2*m))*regularization_term\n",
    "    cost = np.squeeze(cost) \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_backward:\n",
    "Cálculo de la derivada de la función de costo con respecto a las activaciones de la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    dZ -- Gradiente de la función de costo con respecto a la salida pre-activación de las neuronas en la capa actual\n",
    "    cache -- Tupla con (A_prev, W, b) valores guardados durante la ejecución de forwardpropagation \n",
    "\n",
    "    Salidas:\n",
    "    dA_prev -- Gradiente de la función de costo con respecto a las activaciones de las neuronas en la capa anterior (of the previous layer l-1)\n",
    "    dW -- Gradiente de la función de costo con respecto a los pesos de las capas\n",
    "    db -- Gradiente de la función de costo con respecto a los biases de las neuronas en la capa actual\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*(np.dot(dZ,np.transpose(A_prev)))\n",
    "    db = (1/m)*(np.sum(dZ,axis=1,keepdims=True))\n",
    "    dA_prev = np.dot(np.transpose(W),dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_activation_backward: \n",
    "Haciendo uso de la función ***linear_backward*** y de la derivada de la función de costo con respecto a las activaciones de la capa actual, calcula la derivada de la función de costo con respecto a las activaciones de la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    dA -- Gradiente de la función de costo con respecto a las activaciones de la capa actual \n",
    "    cache -- Tupla con (linear_cache, activation_cache), guardados durante forwardpropagation\n",
    "    activation -- Función de activación utilizada en esta capa\n",
    "    \n",
    "    Salidas:\n",
    "    dA_prev -- Gradiente de la función de costo con respecto a las activaciones de la capa anterior\n",
    "    dW -- Gradiente de la función de costo con respecto a los pesos\n",
    "    db -- Gradiente de la función de costo con respecto a los biases\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        rel,cc=activation_function(activation_cache,'relu')\n",
    "        dZ = dA*(rel>=0)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        sig,cc=activation_function(activation_cache,'sigmoid')\n",
    "        dZ = dA*(sig*(1-sig))\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"tanh\":\n",
    "        sig,cc=activation_function(activation_cache,'tanh')\n",
    "        dZ = dA*(sig*(1-sig))\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_model_backward: \n",
    "Cálculo de back-propagation para la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches,act_function1,act_function2):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    AL -- Activaciones de la capa de salida\n",
    "    Y -- Vector de entrada con la clasificación real de cada ejemplo en el set de datos\n",
    "    \n",
    "    Salidas:\n",
    "    grads -- Dictionario con los gradientes de todas las capas\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,act_function2)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "    \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)],current_cache,act_function1)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_parameters: \n",
    "Actualización de parametros utilizando descenso de gradiente estocástico ó adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate,lambd,algorithm,t=0):\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    parameters -- Diccionario con los parámetros de la red \n",
    "    grads -- Diccionario con los gradientes de todas las capas\n",
    "    \n",
    "    Salidas:\n",
    "    parameters -- Diccionario con todos los parámetros de la red actualizados\n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    W1=parameters['W'+str(1)]\n",
    "    m=W1.shape[0]\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    if algorithm == 'sgd':\n",
    "        for l in range(L):\n",
    "            regularization_termW= parameters['W'+str(l+1)]\n",
    "            regularization_termb=parameters['b'+str(l+1)]\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l + 1)]+(lambd/m)*regularization_termW\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l + 1)]+(lambd/m)*regularization_termb\n",
    "    elif algorithm == 'adam':\n",
    "        v, s = initialize_adam(parameters)\n",
    "        parameters,v_,s_=update_parameters_with_adam(parameters,grads,v,s,t,lambd,learning_rate=learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_parameters_with_adam\n",
    "Función utilizada para actualizar los pesos de la red utilizando el algoritmo adam.A continuación se presentan las fórmulas utilizadas en la implementación de adam:\n",
    "$$\\begin{cases}\n",
    "v_{W^{[l]}} = \\beta_1 v_{W^{[l]}} + (1 - \\beta_1) \\frac{\\partial J }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{W^{[l]}} = \\frac{v_{W^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{W^{[l]}} = \\beta_2 s_{W^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{W^{[l]}} = \\frac{s_{W^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{W^{[l]}}}{\\sqrt{s^{corrected}_{W^{[l]}}}+\\varepsilon}\n",
    "\\end{cases}$$\n",
    "\n",
    "Donde $v_{W^{[l]}}$ es el promedio móvil del gradiente (primer momento), y $s_{W^{[l]}}$ es el promedio móvil del gradiente al cuadrado (segundo momento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t,lambd, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parámetros de entrada:\n",
    "    parameters -- Diccionario con los pesos de la red:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- Diccionario con los gradientes de cada capa de la red:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Variable del algoritmo Adam, promedio movil de los gradientes de la red (primer momento)\n",
    "    s -- Variable del algoritmo Adam, promedio movil de los gradientes al cuadrado de la red (segundo momento)\n",
    "    learning_rate -- tasa de aprendizaje.\n",
    "    beta1 -- Hyperparámetro asociado al decrecimiento exponencial de la estimación del primer momento \n",
    "    beta2 -- Hyperparámetro asociado al decrecimiento exponencial de la estimación del segundo momento\n",
    "    epsilon -- Hyperparámetro que previene división por cero en las actualizaciones de los pesos de la red\n",
    "\n",
    "    Salidas:\n",
    "    parameters -- Diccionario con los parametros de la red actualizados \n",
    "    v -- Promedio movil de la estimación del primer momento\n",
    "    s -- Promedio movil de la estimación del segundo momento\n",
    "    \"\"\"\n",
    "    W1=parameters['W'+str(1)]\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                         \n",
    "    m=W1.shape[0]\n",
    "    \n",
    "\n",
    "    for l in range(L):\n",
    "        regularization_termW= parameters['W'+str(l+1)]\n",
    "        regularization_termb=parameters['b'+str(l+1)]\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta1*v['dW'+str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v['db'+str(l+1)]+(1-beta1)*grads['db'+str(l+1)]\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v['dW'+str(l+1)]/(1-np.power(beta1,t))\n",
    "        v_corrected['db'+str(l+1)] = v['db'+str(l+1)]/(1-np.power(beta1,t))\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = beta2*s['dW'+str(l+1)]+(1-beta2)*np.power(grads['dW'+str(l+1)],2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s['db'+str(l+1)]+(1-beta2)*np.power(grads['db'+str(l+1)],2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-np.power(beta2,t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]-learning_rate*(v_corrected['dW'+str(l+1)]/(np.sqrt(s_corrected['dW'+str(l+1)])+epsilon))+(lambd/m)*regularization_termW\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-learning_rate*(v_corrected['db'+str(l+1)]/(np.sqrt(s_corrected['db'+str(l+1)])+epsilon))+(lambd/m)*regularization_termb\n",
    "        \n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_model:\n",
    "Función que calcula los pesos \"óptimos\" de la red neuronal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def L_layer_model(Xt, Yt, layers_dims, learning_rate = 0.0075,learning_rate_type='constant', num_epochs = 100, print_cost=False, lambd=0.001,keep_prob=0.5,act_function1='relu',act_function2='sigmoid',Opt_algorithm='sgd',decay_rate=0.5):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Parámetros de entrada:\n",
    "    Xt -- Datos de entrenamiento\n",
    "    Yt -- Vector de entrada con la clasificación real de cada ejemplo utilizado en el entrenamiento\n",
    "    layers_dims -- Lista con las dimensiones de cada capa de la red.\n",
    "    learning_rate -- Tasa de aprendizaje\n",
    "    learning_rate_type -- Tipo de tasa de aprendizaje utilizada. Puede ser constante con el valor de learning rate ó variable\n",
    "                          siguiendo la formula learning_rate*(1/(1+decay_rate**(#epocasRealizadas)))\n",
    "    Opt_algorithm -- Algoritmo de optimización a utilizar para el proceso de entrenamiento. Puede ser 'sgd' descenso de gradiente estocastico\n",
    "                     ó 'adam'\n",
    "    decay_rate -- tasa de decaimiento utilizada para tasas de aprendizaje variables con respecto al número de epocas\n",
    "    act_function1 -- Función de activación utilizada para las capas escondidas\n",
    "    act_function2 -- Función de activación utilizada para la capa de salida\n",
    "    num_epochs -- Numero de epocas a realizar por el algoritmo de optimización \n",
    "    print_cost -- Si es verdadero, se imprime en consola el valor de la función de costo cada 100 iteracioens\n",
    "    lambd -- parámetro de regularización. Utilizado para regularización L2\n",
    "    keep_prob -- probabilidad de que una neurona viva. Utilizado para regularización con Dropout\n",
    "    \n",
    "    Salidas:\n",
    "    parameters -- parámetros aprendidos por el modelo. \n",
    "    \"\"\"\n",
    "    costs = []                         \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    batch_size=4096\n",
    "    epoch=Xt.shape[0]//batch_size\n",
    "    n_epochs=num_epochs*epoch\n",
    "    for i in range(0, n_epochs):\n",
    "        X=Xt.sample(n=batch_size)\n",
    "        Y=Yt.loc[X.index,0]\n",
    "        X=np.transpose(np.array(X,ndmin=2))\n",
    "        Y=np.array(Y,ndmin=2)\n",
    "        \n",
    "        AL, caches = L_model_forward(X,parameters,keep_prob,act_function1,act_function2)\n",
    "        \n",
    "        cost = compute_cost(AL,Y,lambd,parameters)\n",
    "    \n",
    "        grads = L_model_backward(AL,Y,caches,act_function1,act_function2)\n",
    "        \n",
    "        if learning_rate_type=='invscaling':\n",
    "            lr_decay=learning_rate/(1+decay_rate*(i//epoch))\n",
    "        elif learning_rate_type=='constant':\n",
    "            lr_decay=learning_rate\n",
    "        parameters = update_parameters(parameters,grads,lr_decay,lambd,Opt_algorithm,t=(i+1))\n",
    "        \n",
    "        if print_cost and i % (5*epoch) == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i//epoch, cost))\n",
    "        if print_cost and i % (5*epoch) == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    if print_cost:        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea permite cargar todos los set de datos necesariós para la creación y analisís de los diferentes modelos creados por las redes neuronales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrainw,Ytrainw,Xcvw,Ycvw,Xtestw,Ytestw,XtestT,pca=preprocesing(type='robust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelo y sintonización de hiperparámetros \n",
    "\n",
    "Se decidió sintonizar los parametros en el siguiente orden de importancia: \n",
    "\n",
    "+ tasa de aprendizaje \n",
    "+ Numero de neuronas y capas\n",
    "+ Regularización: $\\lambda$ y keep_prob\n",
    "+ Umbral de selección h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de selección para el parámetro $\\alpha$ o tasa de aprendizaje: \n",
    "\n",
    "Con el fin de seleccionar la mejor tasa de aprendizaje para el problema planteado, se decidío probrar con tasas de aprendizaje variable(que decaen en función del numero de epocas pasadas) y con tasas de aprendizaje constantes. A continuación se presentan los modelos de selección para cada una de ellas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasa de aprendizaje constante: \n",
    "Para seleccionar la mejor tasa de aprendizaje constante se realizarón dos barridos de posibles valores. El primero fue un barrido en escala logarítmica desde $10^{-10}$ hasta $10^{-3}$ para escoger un buen rango de la variable, allí se encontró que el mejor valor de $\\alpha$ para ese rango de valores era $10^{-3}$. Luego, se hizo un barrido para valores cercanos a $10^{-3}$ se tomaron los siguientes valores para el barrido:\n",
    "\n",
    "$$\n",
    "[1*10^{-4},1*10^{-3},1.5*10^{-3},2*10^{-3},3*10^{-3},6*10{-3},7.5*10{-3},9*10{-3},1*10^{-2}]  \n",
    "$$\n",
    "\n",
    "Seguidamente, se calculo el numero de aciertos en el set de validación y se seleccióno el modelo con el porcentaje de aciertos mas alto.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de aprendizaje:  0.0001\n",
      "Tasa de aprendizaje:  0.001\n",
      "Tasa de aprendizaje:  0.0015\n",
      "Tasa de aprendizaje:  0.002\n",
      "Tasa de aprendizaje:  0.003\n",
      "Tasa de aprendizaje:  0.006\n",
      "Tasa de aprendizaje:  0.0075\n",
      "Tasa de aprendizaje:  0.009000000000000001\n",
      "Tasa de aprendizaje:  0.01\n"
     ]
    }
   ],
   "source": [
    "#selección learning rate\n",
    "layer_dims=[Xtrainw.shape[1],50,1]\n",
    "h=0.4\n",
    "Xcv=np.transpose(np.array(Xcvw))\n",
    "Xtrain=np.transpose(np.array(Xtrainw))\n",
    "Ycv=Ycvw\n",
    "Ytrain=Ytrainw\n",
    "#pasoslr=[pow(10,-3),pow(10,-4),pow(10,-5),pow(10,-6),pow(10,-7),pow(10,-8),pow(10,-9),pow(10,-10)]\n",
    "pasoslr=[pow(10,-4),pow(10,-3),1.5*pow(10,-3),2*pow(10,-3),3*pow(10,-3),6*pow(10,-3),7.5*pow(10,-3),9*pow(10,-3),pow(10,-2)]\n",
    "MejorAlphaConstante=0\n",
    "MejorDecayRate=0\n",
    "CalificacionesCVCte=[0]\n",
    "CalificacionesTrainCte=[0]\n",
    "for alpha in pasoslr:\n",
    "    \n",
    "    parameters=L_layer_model(Xtrainw,Ytrainw,layer_dims,print_cost=False,num_epochs=850,keep_prob=1,lambd=0,learning_rate=alpha,Opt_algorithm='adam')\n",
    "    \n",
    "    outCV,cache=L_model_forward(Xcv,parameters,keep_prob=1)\n",
    "    outTrain,cache=L_model_forward(Xtrain,parameters,keep_prob=1)\n",
    "    \n",
    "    predictCV=(outCV>h)*1\n",
    "    predictCV=np.transpose(predictCV)\n",
    "    scoreCV=mes.accuracy_score(Ycv,predictCV)\n",
    "    \n",
    "    predictTrain=(outTrain>h)*1\n",
    "    predictTrain=np.transpose(predictTrain)\n",
    "    scoreTrain=mes.accuracy_score(Ytrain,predictTrain)\n",
    "    \n",
    "    if scoreCV>max(CalificacionesCVCte):\n",
    "        MejorAlphaConstante=alpha\n",
    "        \n",
    "    CalificacionesCVCte.append(scoreCV)\n",
    "    CalificacionesTrainCte.append(scoreTrain)\n",
    "    print('Tasa de aprendizaje: ',alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de la tasa de aprendizaje obtenido:  0.0015\n",
      "Mejor porcentaje de aciertos obtenido en el set de entrenamiento:  0.6287829588258216\n",
      "Mejor porcentaje de aciertos obtenido en el set de validación:  0.6236363636363637\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "indice=pasoslr.index(MejorAlphaConstante)\n",
    "print('Mejor valor de la tasa de aprendizaje obtenido: ',MejorAlphaConstante)\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de entrenamiento: ',max(CalificacionesTrainCte))\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de validación: ',max(CalificacionesCVCte))\n",
    "validacion,=plt.plot(pasoslr,CalificacionesCVCte[1:len(CalificacionesCVCte)],label='Desempeño en el set de validación')\n",
    "entreno,=plt.plot(pasoslr,CalificacionesTrainCte[1:len(CalificacionesTrainCte)],label='Desempeño en el set de entrenamiento')\n",
    "plt.title('Porcentaje de aciertos vs Tasa de aprendizaje')\n",
    "plt.xlabel('Tasa de aprendizaje')\n",
    "plt.ylabel('Porcentaje de aciertos')\n",
    "first_leg=plt.legend(handles=[entreno],loc='upper right')\n",
    "ax=plt.gca().add_artist(first_leg)\n",
    "plt.legend(handles=[validacion],loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasa de aprendizaje variable:\n",
    "<div class=text-justify>\n",
    "Para escoger la mejor tasa de aprendizaje variable, lo ideal sería crear una grilla con todas las posibles combinaciones entre la tasa de aprendizaje y la tasa de decaimiento en un rango de valores apropiado, probarlos todos y utilizar la pareja con mejores resultados en el set de validación. Sin embargo, como dicho cálculo es computacionalmente muy costoso, se decidió establecer un rango de valores para cada una de las variables, y tomar parejas aleatorias y elegir la mejor de entre todas las parejas formadas. Con el fin de tomar una muestra significativa del total de parejas posibles, se tomo la decisión de evaluar cerca del 40% del total de las parejas factibles y escoger la mejor pareja del conjunto evaluado.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion:  0\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.2\n",
      "Iteracion:  1\n",
      "Tasa de aprendizaje:  0.1  Tasa de decaimiento:  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AsusPC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion:  2\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0\n",
      "Iteracion:  3\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0.4\n",
      "Iteracion:  4\n",
      "Tasa de aprendizaje:  0.6000000000000001  Tasa de decaimiento:  0.8\n",
      "Iteracion:  5\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  1\n",
      "Iteracion:  6\n",
      "Tasa de aprendizaje:  0.6000000000000001  Tasa de decaimiento:  0.8\n",
      "Iteracion:  7\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  1\n",
      "Iteracion:  8\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  0.5\n",
      "Iteracion:  9\n",
      "Tasa de aprendizaje:  0.6000000000000001  Tasa de decaimiento:  1\n",
      "Iteracion:  10\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.5\n",
      "Iteracion:  11\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.5\n",
      "Iteracion:  12\n",
      "Tasa de aprendizaje:  0.8  Tasa de decaimiento:  0\n",
      "Iteracion:  13\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.6\n",
      "Iteracion:  14\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0.2\n",
      "Iteracion:  15\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.7\n",
      "Iteracion:  16\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.9\n",
      "Iteracion:  17\n",
      "Tasa de aprendizaje:  0.8  Tasa de decaimiento:  0.2\n",
      "Iteracion:  18\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  1\n",
      "Iteracion:  19\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  0\n",
      "Iteracion:  20\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0.4\n",
      "Iteracion:  21\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.9\n",
      "Iteracion:  22\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0\n",
      "Iteracion:  23\n",
      "Tasa de aprendizaje:  0.1  Tasa de decaimiento:  0.3\n",
      "Iteracion:  24\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0.5\n",
      "Iteracion:  25\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0.2\n",
      "Iteracion:  26\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  1\n",
      "Iteracion:  27\n",
      "Tasa de aprendizaje:  0.6000000000000001  Tasa de decaimiento:  0.6\n",
      "Iteracion:  28\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  0.1\n",
      "Iteracion:  29\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.4\n",
      "Iteracion:  30\n",
      "Tasa de aprendizaje:  0.1  Tasa de decaimiento:  0.5\n",
      "Iteracion:  31\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0.8\n",
      "Iteracion:  32\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.7\n",
      "Iteracion:  33\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.7\n",
      "Iteracion:  34\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.5\n",
      "Iteracion:  35\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0.1\n",
      "Iteracion:  36\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0.3\n",
      "Iteracion:  37\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0.8\n",
      "Iteracion:  38\n",
      "Tasa de aprendizaje:  0.6000000000000001  Tasa de decaimiento:  0.3\n",
      "Iteracion:  39\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.1\n",
      "Iteracion:  40\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  0.8\n",
      "Iteracion:  41\n",
      "Tasa de aprendizaje:  0.4  Tasa de decaimiento:  1\n",
      "Iteracion:  42\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0.6\n",
      "Iteracion:  43\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.8\n",
      "Iteracion:  44\n",
      "Tasa de aprendizaje:  0.8  Tasa de decaimiento:  0.1\n",
      "Iteracion:  45\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  1\n",
      "Iteracion:  46\n",
      "Tasa de aprendizaje:  0.1  Tasa de decaimiento:  0.9\n",
      "Iteracion:  47\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.8\n",
      "Iteracion:  48\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.1\n",
      "Iteracion:  49\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0.1\n",
      "Iteracion:  50\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0\n",
      "Iteracion:  51\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0.1\n",
      "Iteracion:  52\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.5\n",
      "Iteracion:  53\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  1\n",
      "Iteracion:  54\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0.4\n",
      "Iteracion:  55\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0.7\n",
      "Iteracion:  56\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  0.9\n",
      "Iteracion:  57\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  1\n",
      "Iteracion:  58\n",
      "Tasa de aprendizaje:  0.1  Tasa de decaimiento:  0.2\n",
      "Iteracion:  59\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.2\n",
      "Iteracion:  60\n",
      "Tasa de aprendizaje:  0.30000000000000004  Tasa de decaimiento:  0.4\n",
      "Iteracion:  61\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  0.9\n",
      "Iteracion:  62\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0.5\n",
      "Iteracion:  63\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0\n",
      "Iteracion:  64\n",
      "Tasa de aprendizaje:  0.6000000000000001  Tasa de decaimiento:  0.2\n",
      "Iteracion:  65\n",
      "Tasa de aprendizaje:  0.8  Tasa de decaimiento:  0.6\n",
      "Iteracion:  66\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0.3\n",
      "Iteracion:  67\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.6\n",
      "Iteracion:  68\n",
      "Tasa de aprendizaje:  0.01  Tasa de decaimiento:  0.1\n",
      "Iteracion:  69\n",
      "Tasa de aprendizaje:  0.8  Tasa de decaimiento:  0.6\n",
      "Iteracion:  70\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  1\n",
      "Iteracion:  71\n",
      "Tasa de aprendizaje:  0.5  Tasa de decaimiento:  0.2\n",
      "Iteracion:  72\n",
      "Tasa de aprendizaje:  0.8  Tasa de decaimiento:  0.9\n",
      "Iteracion:  73\n",
      "Tasa de aprendizaje:  0.2  Tasa de decaimiento:  0\n",
      "Iteracion:  74\n",
      "Tasa de aprendizaje:  0.7000000000000001  Tasa de decaimiento:  0.7\n"
     ]
    }
   ],
   "source": [
    "#selección learning rate\n",
    "layer_dims=[Xtrainw.shape[1],50,1]\n",
    "h=0.4\n",
    "Xcv=np.transpose(np.array(Xcvw))\n",
    "Xtrain=np.transpose(np.array(Xtrainw))\n",
    "Ycv=Ycvw\n",
    "Ytrain=Ytrainw\n",
    "pasosdr=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "pasoslr=[pow(10,-2),pow(10,-1),2*pow(10,-1),3*pow(10,-1),4*pow(10,-1),5*pow(10,-1),6*pow(10,-1),7*pow(10,-1),8*pow(10,-1)]\n",
    "MejorAlphaVariable=0\n",
    "MejorDecayRate=0\n",
    "CalificacionesCV=[0]\n",
    "CalificacionesTrain=[0]\n",
    "for i in range(75):\n",
    "    \n",
    "    indicelr=np.random.randint(low=0,high=len(pasoslr))\n",
    "    indicedr=np.random.randint(low=0,high=len(pasosdr))\n",
    "    \n",
    "    alpha=pasoslr[indicelr]\n",
    "    decay_rate=pasosdr[indicedr]\n",
    "    \n",
    "    parameters=L_layer_model(Xtrainw,Ytrainw,layer_dims,print_cost=False,num_epochs=850,keep_prob=1,learning_rate_type='invscaling',lambd=0,learning_rate=alpha,decay_rate=decay_rate,Opt_algorithm='adam')\n",
    "    \n",
    "    outCV,cache=L_model_forward(Xcv,parameters,keep_prob=1)\n",
    "    outTrain,cache=L_model_forward(Xtrain,parameters,keep_prob=1)\n",
    "    \n",
    "    predictCV=(outCV>h)*1\n",
    "    predictCV=np.transpose(predictCV)\n",
    "    scoreCV=mes.accuracy_score(Ycv,predictCV)\n",
    "    \n",
    "    predictTrain=(outTrain>h)*1\n",
    "    predictTrain=np.transpose(predictTrain)\n",
    "    scoreTrain=mes.accuracy_score(Ytrain,predictTrain)\n",
    "    \n",
    "    if scoreCV>max(CalificacionesCV):\n",
    "        MejorAlphaVariable=alpha\n",
    "        MejorDecayRate=decay_rate\n",
    "        \n",
    "    CalificacionesCV.append(scoreCV)\n",
    "    CalificacionesTrain.append(scoreTrain)\n",
    "    print('Iteracion: ',i)\n",
    "    print('Tasa de aprendizaje: ',alpha,' Tasa de decaimiento: ',decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de la tasa de aprendizaje obtenido:  0.30000000000000004\n",
      "Mejor valor de la tasa de decaimiento obtenida:  0.4\n",
      "Mejor porcentaje de aciertos obtenido en el set de entrenamiento:  0.6394336926873619\n",
      "Mejor porcentaje de aciertos obtenido en el set de validación:  0.6309090909090909\n"
     ]
    }
   ],
   "source": [
    "# plot\n",
    "indice=pasoslr.index(MejorAlphaVariable)\n",
    "print('Mejor valor de la tasa de aprendizaje obtenido: ',MejorAlphaVariable)\n",
    "print('Mejor valor de la tasa de decaimiento obtenida: ',MejorDecayRate)\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de entrenamiento: ',max(CalificacionesTrain))\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de validación: ',max(CalificacionesCV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es mejor utilizar una tasa de aprendizaje variable por una diferencia de:  0.023636363636363567\n"
     ]
    }
   ],
   "source": [
    "# Este bloque decide si es mejor utilizar una tasa variable o una tasa constante\n",
    "diferencia=np.abs(max(CalificacionesCVCte)-max(CalificacionesCV))\n",
    "if max(CalificacionesCVCte) > max(CalificacionesCV):\n",
    "    MejorAlpha=MejorAlphaConstante\n",
    "    learning_rate_type='constant'\n",
    "    print('Es mejor utilizar una tasa de aprendizaje constante por una diferencia de: ',diferencia)\n",
    "else: \n",
    "    MejorAlpha=MejorAlphaVariable\n",
    "    learning_rate_type='invscaling'\n",
    "    print('Es mejor utilizar una tasa de aprendizaje variable por una diferencia de: ',diferencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimensionesCapasEscondidas: \n",
    "\n",
    "Este función permite generar un arreglo con las dimensiones de las diferentes capas de la red en función de un número de neuronas y número de capas ingresados por parámetro. Esta función asume que todas las capas escondidas de la red tienen el mismo numero de neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dimensionesCapasEscondidas(NumeroNeuronas,NumeroCapas):\n",
    "    layers_dims=np.ones(NumeroCapas)*int(NumeroNeuronas)\n",
    "    layers_dims=np.insert(layers_dims,[0,len(layers_dims)],[Xtrainw.shape[1],1])\n",
    "    layers_dims=layers_dims.astype(int)\n",
    "    return layers_dims.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de selección para el número de neuronas y número de capas en la red\n",
    "\n",
    "Con el fin de escoger el mejor número de neuronas y capas en la red, se utilizó las misma técnica que en el proceso de selección para la tasa de aprendizaje variable. Primero, se establecieron los posibles valores para las variables y se tomaron muestras aleatorias de entre el set de posibles combinaciones. Luego, se elige la mejor combinación de entre todo el set de muestras. \n",
    "    \n",
    "Además, se decidió combinar este paso con el proceso de selección del método de regularización a utilizar, es decir, se agrego una tercera variable que en este casó es la probabilidad de que una neurona sobrevia en el método de Dropout ó el parámetro de regularización L2 $\\lambda$. \n",
    "\n",
    "Dadó que el conjunto de posibles combinaciones es muy grande, se tomo la decisión de tomar 500 muestras y elegir la mejor combinación entre estas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección del mejor número de neuronas, mejor número de capas escondidas y probabilidad de que una nuerona sobreviva en dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración:  0  Probabilidad de neurona viva:  0.2  Numero de neuronas:  7  Numero de capas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AsusPC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración:  10  Probabilidad de neurona viva:  0.4  Numero de neuronas:  7  Numero de capas:  1\n",
      "Iteración:  20  Probabilidad de neurona viva:  0.2  Numero de neuronas:  20  Numero de capas:  2\n",
      "Iteración:  30  Probabilidad de neurona viva:  0.5  Numero de neuronas:  26  Numero de capas:  4\n",
      "Iteración:  40  Probabilidad de neurona viva:  1  Numero de neuronas:  10  Numero de capas:  2\n",
      "Iteración:  50  Probabilidad de neurona viva:  0.1  Numero de neuronas:  23  Numero de capas:  1\n",
      "Iteración:  60  Probabilidad de neurona viva:  0.9  Numero de neuronas:  17  Numero de capas:  2\n",
      "Iteración:  70  Probabilidad de neurona viva:  0.6  Numero de neuronas:  9  Numero de capas:  4\n",
      "Iteración:  80  Probabilidad de neurona viva:  0.3  Numero de neuronas:  11  Numero de capas:  4\n",
      "Iteración:  90  Probabilidad de neurona viva:  1  Numero de neuronas:  4  Numero de capas:  1\n",
      "Iteración:  100  Probabilidad de neurona viva:  0.3  Numero de neuronas:  17  Numero de capas:  1\n",
      "Iteración:  110  Probabilidad de neurona viva:  0.8  Numero de neuronas:  14  Numero de capas:  3\n",
      "Iteración:  120  Probabilidad de neurona viva:  0.2  Numero de neuronas:  2  Numero de capas:  1\n",
      "Iteración:  130  Probabilidad de neurona viva:  0.3  Numero de neuronas:  20  Numero de capas:  4\n",
      "Iteración:  140  Probabilidad de neurona viva:  0.3  Numero de neuronas:  1  Numero de capas:  2\n",
      "Iteración:  150  Probabilidad de neurona viva:  1  Numero de neuronas:  2  Numero de capas:  4\n",
      "Iteración:  160  Probabilidad de neurona viva:  0.4  Numero de neuronas:  29  Numero de capas:  3\n",
      "Iteración:  170  Probabilidad de neurona viva:  0.9  Numero de neuronas:  8  Numero de capas:  2\n",
      "Iteración:  180  Probabilidad de neurona viva:  0.6  Numero de neuronas:  9  Numero de capas:  1\n",
      "Iteración:  190  Probabilidad de neurona viva:  0.7  Numero de neuronas:  11  Numero de capas:  4\n",
      "Iteración:  200  Probabilidad de neurona viva:  0.1  Numero de neuronas:  5  Numero de capas:  2\n",
      "Iteración:  210  Probabilidad de neurona viva:  0.7  Numero de neuronas:  8  Numero de capas:  1\n",
      "Iteración:  220  Probabilidad de neurona viva:  1  Numero de neuronas:  1  Numero de capas:  1\n",
      "Iteración:  230  Probabilidad de neurona viva:  0.5  Numero de neuronas:  29  Numero de capas:  2\n",
      "Iteración:  240  Probabilidad de neurona viva:  0.6  Numero de neuronas:  8  Numero de capas:  2\n",
      "Iteración:  250  Probabilidad de neurona viva:  0.6  Numero de neuronas:  10  Numero de capas:  3\n",
      "Iteración:  260  Probabilidad de neurona viva:  0.9  Numero de neuronas:  4  Numero de capas:  1\n",
      "Iteración:  270  Probabilidad de neurona viva:  0.4  Numero de neuronas:  2  Numero de capas:  4\n",
      "Iteración:  280  Probabilidad de neurona viva:  0.2  Numero de neuronas:  9  Numero de capas:  3\n",
      "Iteración:  290  Probabilidad de neurona viva:  0.2  Numero de neuronas:  6  Numero de capas:  2\n",
      "Iteración:  300  Probabilidad de neurona viva:  0.6  Numero de neuronas:  9  Numero de capas:  4\n",
      "Iteración:  310  Probabilidad de neurona viva:  0.2  Numero de neuronas:  1  Numero de capas:  1\n",
      "Iteración:  320  Probabilidad de neurona viva:  0.9  Numero de neuronas:  11  Numero de capas:  4\n",
      "Iteración:  330  Probabilidad de neurona viva:  0.7  Numero de neuronas:  8  Numero de capas:  2\n",
      "Iteración:  340  Probabilidad de neurona viva:  0.1  Numero de neuronas:  6  Numero de capas:  3\n",
      "Iteración:  350  Probabilidad de neurona viva:  0.3  Numero de neuronas:  3  Numero de capas:  3\n",
      "Iteración:  360  Probabilidad de neurona viva:  0.6  Numero de neuronas:  10  Numero de capas:  4\n",
      "Iteración:  370  Probabilidad de neurona viva:  1  Numero de neuronas:  26  Numero de capas:  3\n",
      "Iteración:  380  Probabilidad de neurona viva:  0.3  Numero de neuronas:  10  Numero de capas:  4\n",
      "Iteración:  390  Probabilidad de neurona viva:  0.5  Numero de neuronas:  4  Numero de capas:  2\n",
      "Iteración:  400  Probabilidad de neurona viva:  0.7  Numero de neuronas:  9  Numero de capas:  1\n",
      "Iteración:  410  Probabilidad de neurona viva:  0.9  Numero de neuronas:  3  Numero de capas:  4\n",
      "Iteración:  420  Probabilidad de neurona viva:  1  Numero de neuronas:  3  Numero de capas:  2\n",
      "Iteración:  430  Probabilidad de neurona viva:  0.1  Numero de neuronas:  26  Numero de capas:  2\n",
      "Iteración:  440  Probabilidad de neurona viva:  0.4  Numero de neuronas:  7  Numero de capas:  2\n",
      "Iteración:  450  Probabilidad de neurona viva:  0.6  Numero de neuronas:  7  Numero de capas:  4\n",
      "Iteración:  460  Probabilidad de neurona viva:  0.1  Numero de neuronas:  7  Numero de capas:  1\n",
      "Iteración:  470  Probabilidad de neurona viva:  0.6  Numero de neuronas:  7  Numero de capas:  3\n",
      "Iteración:  480  Probabilidad de neurona viva:  1  Numero de neuronas:  3  Numero de capas:  3\n",
      "Iteración:  490  Probabilidad de neurona viva:  0.3  Numero de neuronas:  10  Numero de capas:  1\n"
     ]
    }
   ],
   "source": [
    "# selección kp, NumNeuronas y numero de capas\n",
    "layer_dims=[Xtrainw.shape[1],50,1]\n",
    "h=0.4\n",
    "Xcv=np.transpose(np.array(Xcvw))\n",
    "Xtrain=np.transpose(np.array(Xtrainw))\n",
    "Ycv=Ycvw\n",
    "Ytrain=Ytrainw\n",
    "pasoskp=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "pasosNn=list(range(1,11))+list(range(11,31,3))\n",
    "pasosNc=range(1,5)\n",
    "Mejorkp=0\n",
    "MejorNnKp=0\n",
    "MejorNcKp=0\n",
    "CalificacionesCVkp=[0]\n",
    "CalificacionesTrainkp=[0]\n",
    "for i in range(500):\n",
    "    \n",
    "    indicekp=np.random.randint(low=0,high=len(pasoskp))\n",
    "    indiceNn=np.random.randint(low=0,high=len(pasosNn))\n",
    "    indiceNc=np.random.randint(low=0,high=len(pasosNc))\n",
    "    \n",
    "    keep_prob=pasoskp[indicekp]\n",
    "    NumNeuronas=pasosNn[indiceNn]\n",
    "    NumCapas=pasosNc[indiceNc]\n",
    "    \n",
    "    layer_dims=dimensionesCapasEscondidas(NumNeuronas,NumCapas)\n",
    "    parameters=L_layer_model(Xtrainw,Ytrainw,layer_dims,print_cost=False,num_epochs=850,keep_prob=keep_prob,lambd=0,learning_rate_type=learning_rate_type,learning_rate=MejorAlpha,decay_rate=MejorDecayRate,Opt_algorithm='adam')\n",
    "    \n",
    "    outCV,cache=L_model_forward(Xcv,parameters,keep_prob=1)\n",
    "    outTrain,cache=L_model_forward(Xtrain,parameters,keep_prob=1)\n",
    "    \n",
    "    predictCV=(outCV>h)*1\n",
    "    predictCV=np.transpose(predictCV)\n",
    "    scoreCV=mes.accuracy_score(Ycv,predictCV)\n",
    "    \n",
    "    predictTrain=(outTrain>h)*1\n",
    "    predictTrain=np.transpose(predictTrain)\n",
    "    scoreTrain=mes.accuracy_score(Ytrain,predictTrain)\n",
    "    \n",
    "    if scoreCV>max(CalificacionesCVkp):\n",
    "        Mejorkp=keep_prob\n",
    "        MejorNnKp=NumNeuronas\n",
    "        MejorNcKp=NumCapas\n",
    "        \n",
    "    CalificacionesCVkp.append(scoreCV)\n",
    "    CalificacionesTrainkp.append(scoreTrain)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Iteración: ',i,' Probabilidad de neurona viva: ',keep_prob,' Numero de neuronas: ',NumNeuronas,' Numero de capas: ',NumCapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de la probabilidad de que una neurona sobreviva en Dropout:  0.3\n",
      "Mejor numero de neuronas:  26\n",
      "Mejor numero de capas:  1\n",
      "Mejor porcentaje de aciertos obtenido en el set de entrenamiento:  0.6331991167684115\n",
      "Mejor porcentaje de aciertos obtenido en el set de validación:  0.6351515151515151\n"
     ]
    }
   ],
   "source": [
    "#plot \n",
    "print('Mejor valor de la probabilidad de que una neurona sobreviva en Dropout: ',Mejorkp)\n",
    "print('Mejor numero de neuronas: ',MejorNnKp)\n",
    "print('Mejor numero de capas: ', MejorNcKp)\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de entrenamiento: ',max(CalificacionesTrainkp))\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de validación: ',max(CalificacionesCVkp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección del mejor número de neuronas, mejor número de capas escondidas y  parámetro de regularización L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración:  0  Parámetro de regularización L2:  0.005  Numero de neuronas:  5  Numero de capas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AsusPC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración:  10  Parámetro de regularización L2:  0.0075  Numero de neuronas:  8  Numero de capas:  2\n",
      "Iteración:  20  Parámetro de regularización L2:  0.0085  Numero de neuronas:  7  Numero de capas:  3\n",
      "Iteración:  30  Parámetro de regularización L2:  0.0001  Numero de neuronas:  6  Numero de capas:  1\n",
      "Iteración:  40  Parámetro de regularización L2:  0.001  Numero de neuronas:  8  Numero de capas:  2\n",
      "Iteración:  50  Parámetro de regularización L2:  0.003  Numero de neuronas:  21  Numero de capas:  3\n",
      "Iteración:  60  Parámetro de regularización L2:  0.01  Numero de neuronas:  21  Numero de capas:  3\n",
      "Iteración:  70  Parámetro de regularización L2:  0.005  Numero de neuronas:  16  Numero de capas:  3\n",
      "Iteración:  80  Parámetro de regularización L2:  0.005  Numero de neuronas:  6  Numero de capas:  3\n",
      "Iteración:  90  Parámetro de regularización L2:  0.005  Numero de neuronas:  4  Numero de capas:  2\n",
      "Iteración:  100  Parámetro de regularización L2:  0.0075  Numero de neuronas:  5  Numero de capas:  1\n",
      "Iteración:  110  Parámetro de regularización L2:  0.001  Numero de neuronas:  2  Numero de capas:  1\n",
      "Iteración:  120  Parámetro de regularización L2:  0.005  Numero de neuronas:  16  Numero de capas:  1\n",
      "Iteración:  130  Parámetro de regularización L2:  0.0075  Numero de neuronas:  21  Numero de capas:  1\n",
      "Iteración:  140  Parámetro de regularización L2:  0.003  Numero de neuronas:  26  Numero de capas:  2\n",
      "Iteración:  150  Parámetro de regularización L2:  0.01  Numero de neuronas:  16  Numero de capas:  2\n",
      "Iteración:  160  Parámetro de regularización L2:  0.0075  Numero de neuronas:  9  Numero de capas:  2\n",
      "Iteración:  170  Parámetro de regularización L2:  0.01  Numero de neuronas:  1  Numero de capas:  1\n",
      "Iteración:  180  Parámetro de regularización L2:  0.003  Numero de neuronas:  6  Numero de capas:  3\n",
      "Iteración:  190  Parámetro de regularización L2:  0.0001  Numero de neuronas:  9  Numero de capas:  2\n",
      "Iteración:  200  Parámetro de regularización L2:  0.0075  Numero de neuronas:  11  Numero de capas:  1\n",
      "Iteración:  210  Parámetro de regularización L2:  0.005  Numero de neuronas:  1  Numero de capas:  2\n",
      "Iteración:  220  Parámetro de regularización L2:  0.0085  Numero de neuronas:  16  Numero de capas:  1\n",
      "Iteración:  230  Parámetro de regularización L2:  0.0085  Numero de neuronas:  4  Numero de capas:  1\n",
      "Iteración:  240  Parámetro de regularización L2:  0.0001  Numero de neuronas:  6  Numero de capas:  3\n",
      "Iteración:  250  Parámetro de regularización L2:  0.0075  Numero de neuronas:  4  Numero de capas:  1\n",
      "Iteración:  260  Parámetro de regularización L2:  0.003  Numero de neuronas:  26  Numero de capas:  2\n",
      "Iteración:  270  Parámetro de regularización L2:  0.0001  Numero de neuronas:  5  Numero de capas:  3\n",
      "Iteración:  280  Parámetro de regularización L2:  0.005  Numero de neuronas:  1  Numero de capas:  1\n",
      "Iteración:  290  Parámetro de regularización L2:  0.0075  Numero de neuronas:  16  Numero de capas:  3\n",
      "Iteración:  300  Parámetro de regularización L2:  0.003  Numero de neuronas:  8  Numero de capas:  2\n",
      "Iteración:  310  Parámetro de regularización L2:  0.005  Numero de neuronas:  1  Numero de capas:  1\n",
      "Iteración:  320  Parámetro de regularización L2:  0.001  Numero de neuronas:  11  Numero de capas:  3\n",
      "Iteración:  330  Parámetro de regularización L2:  0.01  Numero de neuronas:  4  Numero de capas:  2\n",
      "Iteración:  340  Parámetro de regularización L2:  0.005  Numero de neuronas:  4  Numero de capas:  1\n",
      "Iteración:  350  Parámetro de regularización L2:  0.001  Numero de neuronas:  26  Numero de capas:  3\n",
      "Iteración:  360  Parámetro de regularización L2:  0.001  Numero de neuronas:  26  Numero de capas:  1\n",
      "Iteración:  370  Parámetro de regularización L2:  0.005  Numero de neuronas:  21  Numero de capas:  2\n",
      "Iteración:  380  Parámetro de regularización L2:  0.0001  Numero de neuronas:  11  Numero de capas:  1\n",
      "Iteración:  390  Parámetro de regularización L2:  0.0001  Numero de neuronas:  1  Numero de capas:  3\n",
      "Iteración:  400  Parámetro de regularización L2:  0.0075  Numero de neuronas:  5  Numero de capas:  1\n",
      "Iteración:  410  Parámetro de regularización L2:  0.001  Numero de neuronas:  7  Numero de capas:  3\n",
      "Iteración:  420  Parámetro de regularización L2:  0.003  Numero de neuronas:  21  Numero de capas:  3\n",
      "Iteración:  430  Parámetro de regularización L2:  0.001  Numero de neuronas:  2  Numero de capas:  1\n",
      "Iteración:  440  Parámetro de regularización L2:  0.0085  Numero de neuronas:  2  Numero de capas:  2\n",
      "Iteración:  450  Parámetro de regularización L2:  0.01  Numero de neuronas:  10  Numero de capas:  1\n",
      "Iteración:  460  Parámetro de regularización L2:  0.0085  Numero de neuronas:  1  Numero de capas:  3\n",
      "Iteración:  470  Parámetro de regularización L2:  0.005  Numero de neuronas:  3  Numero de capas:  2\n",
      "Iteración:  480  Parámetro de regularización L2:  0.0001  Numero de neuronas:  21  Numero de capas:  2\n",
      "Iteración:  490  Parámetro de regularización L2:  0.01  Numero de neuronas:  8  Numero de capas:  2\n"
     ]
    }
   ],
   "source": [
    "# selección lambda, NumNeuronas y numero de capas\n",
    "layer_dims=[Xtrainw.shape[1],50,1]\n",
    "h=0.4\n",
    "Xcv=np.transpose(np.array(Xcvw))\n",
    "Xtrain=np.transpose(np.array(Xtrainw))\n",
    "Ycv=Ycvw\n",
    "Ytrain=Ytrainw\n",
    "pasosL2=[pow(10,-4),pow(10,-3),3*pow(10,-3),5*pow(10,-3),7.5*pow(10,-3),8.5*pow(10,-3),pow(10,-2)]\n",
    "pasosNn=list(range(1,11))+list(range(11,31,5))\n",
    "pasosNc=range(1,4)\n",
    "MejorLambda=0\n",
    "MejorNnL=0\n",
    "MejorNcL=0\n",
    "CalificacionesCVl2=[0]\n",
    "CalificacionesTrainl2=[0]\n",
    "for i in range(500):\n",
    "    \n",
    "    indicel2=np.random.randint(low=0,high=len(pasosL2))\n",
    "    indiceNnL=np.random.randint(low=0,high=len(pasosNn))\n",
    "    indiceNcL=np.random.randint(low=0,high=len(pasosNc))\n",
    "    \n",
    "    lambd=pasosL2[indicel2]\n",
    "    NumNeuronas=pasosNn[indiceNnL]\n",
    "    NumCapas=pasosNc[indiceNcL]\n",
    "    \n",
    "    layer_dims=dimensionesCapasEscondidas(NumNeuronas,NumCapas)\n",
    "    parameters=L_layer_model(Xtrainw,Ytrainw,layer_dims,print_cost=False,num_epochs=850,keep_prob=1,learning_rate_type=learning_rate_type,lambd=lambd,learning_rate=MejorAlpha,decay_rate=MejorDecayRate,Opt_algorithm='adam')\n",
    "    \n",
    "    outCV,cache=L_model_forward(Xcv,parameters,keep_prob=1)\n",
    "    outTrain,cache=L_model_forward(Xtrain,parameters,keep_prob=1)\n",
    "    \n",
    "    predictCV=(outCV>h)*1\n",
    "    predictCV=np.transpose(predictCV)\n",
    "    scoreCV=mes.accuracy_score(Ycv,predictCV)\n",
    "    \n",
    "    predictTrain=(outTrain>h)*1\n",
    "    predictTrain=np.transpose(predictTrain)\n",
    "    scoreTrain=mes.accuracy_score(Ytrain,predictTrain)\n",
    "    \n",
    "    if scoreCV>max(CalificacionesCVl2):\n",
    "        MejorLambda=lambd\n",
    "        MejorNnL=NumNeuronas\n",
    "        MejorNcL=NumCapas\n",
    "        \n",
    "    CalificacionesCVl2.append(scoreCV)\n",
    "    CalificacionesTrainl2.append(scoreTrain)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Iteración: ',i,' Parámetro de regularización L2: ',lambd,' Numero de neuronas: ',NumNeuronas,' Numero de capas: ',NumCapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de la parámetro de regularización hallado:  0.01\n",
      "Mejor numero de neuronas:  9\n",
      "Mejor numero de capas:  1\n",
      "Mejor porcentaje de aciertos obtenido en el set de entrenamiento:  0.6393038056890505\n",
      "Mejor porcentaje de aciertos obtenido en el set de validación:  0.6339393939393939\n"
     ]
    }
   ],
   "source": [
    "#plot \n",
    "print('Mejor valor de la parámetro de regularización hallado: ',MejorLambda)\n",
    "print('Mejor numero de neuronas: ',MejorNnL)\n",
    "print('Mejor numero de capas: ', MejorNcL)\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de entrenamiento: ',max(CalificacionesTrainl2))\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de validación: ',max(CalificacionesCVl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es mejor utilizar como método de regularización la regularización por Dropout por una diferencia de:  0.00121212121212122\n"
     ]
    }
   ],
   "source": [
    "diferencia=np.abs(max(CalificacionesCVkp)-max(CalificacionesCVl2))\n",
    "if max(CalificacionesCVkp)>max(CalificacionesCVl2):\n",
    "    MejorLambda=0\n",
    "    MejorNn=MejorNnKp\n",
    "    MejorNc=MejorNcKp\n",
    "    print('Es mejor utilizar como método de regularización la regularización por Dropout por una diferencia de: ',diferencia)\n",
    "else:\n",
    "    Mejorkp=1\n",
    "    MejorNn=MejorNnL\n",
    "    MejorNc=MejorNcL\n",
    "    print('Es mejor utilizar como método de regularización la regularización por L2 por una diferencia de: ',diferencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de selección para el umbral de selección (h): \n",
    "\n",
    "Para seleccionar el umbral de selección (h) se realizó un barrido desde 0.45 hasta 0.56 con pasos de 0.1 y se eligió el h que tuviera el mejor resultado en el set de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral actual:  0.45\n",
      "Umbral actual:  0.46\n",
      "Umbral actual:  0.47\n",
      "Umbral actual:  0.48\n",
      "Umbral actual:  0.49\n",
      "Umbral actual:  0.5\n",
      "Umbral actual:  0.51\n",
      "Umbral actual:  0.52\n",
      "Umbral actual:  0.53\n",
      "Umbral actual:  0.54\n",
      "Umbral actual:  0.55\n",
      "Umbral actual:  0.56\n"
     ]
    }
   ],
   "source": [
    "# selección umbral\n",
    "layer_dims=dimensionesCapasEscondidas(MejorNn,MejorNc)\n",
    "Xcv=np.transpose(np.array(Xcvw))\n",
    "Xtrain=np.transpose(np.array(Xtrainw))\n",
    "Ycv=Ycvw\n",
    "Ytrain=Ytrainw\n",
    "pasosh=[0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56]\n",
    "MejorUmbral=0\n",
    "CalificacionesCV=[0]\n",
    "CalificacionesTrain=[0]\n",
    "for h in pasosh:\n",
    "    parameters=L_layer_model(Xtrainw,Ytrainw,layer_dims,print_cost=False,num_epochs=850,keep_prob=Mejorkp,lambd=0,learning_rate_type=learning_rate_type,learning_rate=MejorAlpha,decay_rate=MejorDecayRate,Opt_algorithm='adam')\n",
    "    \n",
    "    outCV,cache=L_model_forward(Xcv,parameters,keep_prob=1)\n",
    "    outTrain,cache=L_model_forward(Xtrain,parameters,keep_prob=1)\n",
    "    \n",
    "    predictCV=(outCV>h)*1\n",
    "    predictCV=np.transpose(predictCV)\n",
    "    scoreCV=mes.accuracy_score(Ycv,predictCV)\n",
    "    \n",
    "    predictTrain=(outTrain>h)*1\n",
    "    predictTrain=np.transpose(predictTrain)\n",
    "    scoreTrain=mes.accuracy_score(Ytrain,predictTrain)\n",
    "    \n",
    "    if scoreCV>max(CalificacionesCV):\n",
    "        MejorUmbral=h\n",
    "        \n",
    "    CalificacionesCV.append(scoreCV)\n",
    "    CalificacionesTrain.append(scoreTrain)\n",
    "    \n",
    "    print('Umbral actual: ',h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor umbral de selección obtenido:  0.48\n",
      "Mejor porcentaje de aciertos obtenido en el set de entrenamiento:  0.6554097934796727\n",
      "Mejor porcentaje de aciertos obtenido en el set de validación:  0.6472727272727272\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "indice=pasosh.index(MejorUmbral)\n",
    "print('Mejor umbral de selección obtenido: ',MejorUmbral)\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de entrenamiento: ',max(CalificacionesTrain))\n",
    "print('Mejor porcentaje de aciertos obtenido en el set de validación: ',max(CalificacionesCV))\n",
    "validacion,=plt.plot(pasosh,CalificacionesCV[1:len(CalificacionesCV)],label='Desempeño en el set de validación')\n",
    "entreno,=plt.plot(pasosh,CalificacionesTrain[1:len(CalificacionesTrain)],label='Desempeño en el set de entrenamiento')\n",
    "plt.title('Porcentaje de aciertos vs Tasa de aprendizaje')\n",
    "plt.xlabel('Tasa de aprendizaje')\n",
    "plt.ylabel('Porcentaje de aciertos')\n",
    "first_leg=plt.legend(handles=[entreno],loc='upper right')\n",
    "ax=plt.gca().add_artist(first_leg)\n",
    "plt.legend(handles=[validacion],loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejor modelo encontrado\n",
    "Para entrenar al modelo final se utilizó la sintonización de los hyperparámetros realizada anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.948773\n",
      "Cost after epoch 5: 0.788445\n",
      "Cost after epoch 10: 0.695477\n",
      "Cost after epoch 15: 0.682595\n",
      "Cost after epoch 20: 0.671807\n",
      "Cost after epoch 25: 0.659159\n",
      "Cost after epoch 30: 0.662735\n",
      "Cost after epoch 35: 0.658314\n",
      "Cost after epoch 40: 0.652716\n",
      "Cost after epoch 45: 0.650140\n",
      "Cost after epoch 50: 0.652091\n",
      "Cost after epoch 55: 0.641134\n",
      "Cost after epoch 60: 0.638684\n",
      "Cost after epoch 65: 0.644902\n",
      "Cost after epoch 70: 0.639361\n",
      "Cost after epoch 75: 0.636800\n",
      "Cost after epoch 80: 0.637960\n",
      "Cost after epoch 85: 0.631052\n",
      "Cost after epoch 90: 0.634403\n",
      "Cost after epoch 95: 0.635677\n",
      "Cost after epoch 100: 0.631282\n",
      "Cost after epoch 105: 0.628978\n",
      "Cost after epoch 110: 0.635114\n",
      "Cost after epoch 115: 0.630695\n",
      "Cost after epoch 120: 0.633726\n",
      "Cost after epoch 125: 0.632322\n",
      "Cost after epoch 130: 0.633128\n",
      "Cost after epoch 135: 0.635127\n",
      "Cost after epoch 140: 0.630614\n",
      "Cost after epoch 145: 0.636312\n",
      "Cost after epoch 150: 0.629161\n",
      "Cost after epoch 155: 0.628041\n",
      "Cost after epoch 160: 0.625210\n",
      "Cost after epoch 165: 0.631700\n",
      "Cost after epoch 170: 0.633109\n",
      "Cost after epoch 175: 0.622770\n",
      "Cost after epoch 180: 0.631185\n",
      "Cost after epoch 185: 0.634646\n",
      "Cost after epoch 190: 0.629268\n",
      "Cost after epoch 195: 0.631046\n",
      "Cost after epoch 200: 0.629194\n",
      "Cost after epoch 205: 0.632347\n",
      "Cost after epoch 210: 0.626150\n",
      "Cost after epoch 215: 0.625033\n",
      "Cost after epoch 220: 0.622748\n",
      "Cost after epoch 225: 0.630967\n",
      "Cost after epoch 230: 0.627616\n",
      "Cost after epoch 235: 0.628662\n",
      "Cost after epoch 240: 0.623892\n",
      "Cost after epoch 245: 0.616974\n",
      "Cost after epoch 250: 0.625112\n",
      "Cost after epoch 255: 0.630207\n",
      "Cost after epoch 260: 0.627441\n",
      "Cost after epoch 265: 0.623095\n",
      "Cost after epoch 270: 0.622339\n",
      "Cost after epoch 275: 0.626420\n",
      "Cost after epoch 280: 0.628623\n",
      "Cost after epoch 285: 0.625967\n",
      "Cost after epoch 290: 0.627087\n",
      "Cost after epoch 295: 0.625637\n",
      "Cost after epoch 300: 0.627902\n",
      "Cost after epoch 305: 0.626897\n",
      "Cost after epoch 310: 0.625438\n",
      "Cost after epoch 315: 0.623763\n",
      "Cost after epoch 320: 0.621526\n",
      "Cost after epoch 325: 0.622544\n",
      "Cost after epoch 330: 0.625481\n",
      "Cost after epoch 335: 0.629570\n",
      "Cost after epoch 340: 0.623522\n",
      "Cost after epoch 345: 0.627614\n",
      "Cost after epoch 350: 0.630424\n",
      "Cost after epoch 355: 0.627289\n",
      "Cost after epoch 360: 0.629335\n",
      "Cost after epoch 365: 0.631829\n",
      "Cost after epoch 370: 0.631857\n",
      "Cost after epoch 375: 0.621830\n",
      "Cost after epoch 380: 0.624011\n",
      "Cost after epoch 385: 0.625286\n",
      "Cost after epoch 390: 0.630550\n",
      "Cost after epoch 395: 0.628693\n",
      "Cost after epoch 400: 0.622250\n",
      "Cost after epoch 405: 0.624367\n",
      "Cost after epoch 410: 0.626098\n",
      "Cost after epoch 415: 0.625913\n",
      "Cost after epoch 420: 0.618732\n",
      "Cost after epoch 425: 0.627589\n",
      "Cost after epoch 430: 0.626440\n",
      "Cost after epoch 435: 0.622701\n",
      "Cost after epoch 440: 0.626343\n",
      "Cost after epoch 445: 0.624966\n",
      "Cost after epoch 450: 0.623051\n",
      "Cost after epoch 455: 0.625928\n",
      "Cost after epoch 460: 0.635376\n",
      "Cost after epoch 465: 0.626167\n",
      "Cost after epoch 470: 0.623894\n",
      "Cost after epoch 475: 0.620169\n",
      "Cost after epoch 480: 0.619389\n",
      "Cost after epoch 485: 0.620921\n",
      "Cost after epoch 490: 0.631857\n",
      "Cost after epoch 495: 0.625057\n",
      "Cost after epoch 500: 0.619136\n",
      "Cost after epoch 505: 0.618475\n",
      "Cost after epoch 510: 0.623792\n",
      "Cost after epoch 515: 0.626036\n",
      "Cost after epoch 520: 0.623015\n",
      "Cost after epoch 525: 0.626497\n",
      "Cost after epoch 530: 0.630225\n",
      "Cost after epoch 535: 0.633176\n",
      "Cost after epoch 540: 0.626845\n",
      "Cost after epoch 545: 0.622360\n",
      "Cost after epoch 550: 0.625437\n",
      "Cost after epoch 555: 0.623626\n",
      "Cost after epoch 560: 0.625764\n",
      "Cost after epoch 565: 0.620302\n",
      "Cost after epoch 570: 0.623772\n",
      "Cost after epoch 575: 0.625217\n",
      "Cost after epoch 580: 0.621837\n",
      "Cost after epoch 585: 0.621501\n",
      "Cost after epoch 590: 0.622661\n",
      "Cost after epoch 595: 0.628430\n",
      "Cost after epoch 600: 0.619547\n",
      "Cost after epoch 605: 0.622747\n",
      "Cost after epoch 610: 0.619094\n",
      "Cost after epoch 615: 0.623210\n",
      "Cost after epoch 620: 0.627835\n",
      "Cost after epoch 625: 0.630762\n",
      "Cost after epoch 630: 0.619604\n",
      "Cost after epoch 635: 0.624228\n",
      "Cost after epoch 640: 0.617023\n",
      "Cost after epoch 645: 0.622417\n",
      "Cost after epoch 650: 0.619935\n",
      "Cost after epoch 655: 0.628723\n",
      "Cost after epoch 660: 0.624519\n",
      "Cost after epoch 665: 0.621989\n",
      "Cost after epoch 670: 0.621435\n",
      "Cost after epoch 675: 0.624349\n",
      "Cost after epoch 680: 0.628826\n",
      "Cost after epoch 685: 0.622355\n",
      "Cost after epoch 690: 0.625023\n",
      "Cost after epoch 695: 0.623636\n",
      "Cost after epoch 700: 0.622796\n",
      "Cost after epoch 705: 0.619792\n",
      "Cost after epoch 710: 0.615377\n",
      "Cost after epoch 715: 0.627604\n",
      "Cost after epoch 720: 0.622390\n",
      "Cost after epoch 725: 0.626212\n",
      "Cost after epoch 730: 0.625329\n",
      "Cost after epoch 735: 0.628480\n",
      "Cost after epoch 740: 0.618781\n",
      "Cost after epoch 745: 0.623967\n",
      "Cost after epoch 750: 0.625503\n",
      "Cost after epoch 755: 0.618468\n",
      "Cost after epoch 760: 0.620149\n",
      "Cost after epoch 765: 0.617935\n",
      "Cost after epoch 770: 0.629261\n",
      "Cost after epoch 775: 0.617145\n",
      "Cost after epoch 780: 0.620625\n",
      "Cost after epoch 785: 0.628968\n",
      "Cost after epoch 790: 0.622510\n",
      "Cost after epoch 795: 0.626893\n",
      "Cost after epoch 800: 0.625482\n",
      "Cost after epoch 805: 0.623329\n",
      "Cost after epoch 810: 0.627165\n",
      "Cost after epoch 815: 0.621664\n",
      "Cost after epoch 820: 0.621364\n",
      "Cost after epoch 825: 0.623283\n",
      "Cost after epoch 830: 0.621825\n",
      "Cost after epoch 835: 0.619657\n",
      "Cost after epoch 840: 0.622818\n",
      "Cost after epoch 845: 0.621271\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del mejor modelo en el set de entrenamiento: \n",
      "Porcentaje de aciertos:  0.6486556695674763\n",
      "\n",
      "\n",
      "Recall,precision y valor-F del modelo\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " partícula 1       0.67      0.53      0.59      3666\n",
      " partícula 2       0.64      0.76      0.69      4033\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      7699\n",
      "   macro avg       0.65      0.64      0.64      7699\n",
      "weighted avg       0.65      0.65      0.64      7699\n",
      "\n",
      "\n",
      "\n",
      "Resultados del mejor modelo en el set de validación: \n",
      "Porcentaje de aciertos:  0.636969696969697\n",
      "\n",
      "\n",
      "Recall,precision y valor-F del modelo\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " partícula 1       0.67      0.51      0.57       800\n",
      " partícula 2       0.62      0.76      0.68       850\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1650\n",
      "   macro avg       0.64      0.63      0.63      1650\n",
      "weighted avg       0.64      0.64      0.63      1650\n",
      "\n",
      "\n",
      "\n",
      "Resultados del mejor modelo en el set de prueba: \n",
      "Porcentaje de aciertos:  0.6656571774682011\n",
      "\n",
      "\n",
      "Recall,precision y valor-F del modelo\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " partícula 1       0.69      0.54      0.61       786\n",
      " partícula 2       0.65      0.78      0.71       865\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1651\n",
      "   macro avg       0.67      0.66      0.66      1651\n",
      "weighted avg       0.67      0.67      0.66      1651\n",
      "\n",
      "\n",
      "\n",
      "Parámetros utilizados para el mejor modelo: \n",
      "Mejor tasa de aprendizaje:  0.30000000000000004\n",
      "Mejor tasa de decaimiento:  0.4\n",
      "Mejor Número de neuronas por capa:  26\n",
      "Mejor Número de capas:  1\n",
      "Mejor probabilidad de dejar neuronas vivas:  0.3\n",
      "Mejor parámetro de regularización L2:  0\n",
      "Mejor umbral de selección:  0.48\n"
     ]
    }
   ],
   "source": [
    "layer_dims=dimensionesCapasEscondidas(MejorNn,MejorNc)\n",
    "Xcv=np.transpose(np.array(Xcvw))\n",
    "Xtrain=np.transpose(np.array(Xtrainw))\n",
    "Xtest=np.transpose(np.array(Xtestw))\n",
    "Ycv=Ycvw\n",
    "Ytrain=Ytrainw\n",
    "Ytest=Ytestw\n",
    "\n",
    "parameters=L_layer_model(Xtrainw,Ytrainw,layer_dims,print_cost=True,learning_rate_type=learning_rate_type,num_epochs=850,keep_prob=Mejorkp,lambd=MejorLambda,learning_rate=MejorAlpha,decay_rate=MejorDecayRate,Opt_algorithm='adam')\n",
    "    \n",
    "outCV,cache=L_model_forward(Xcv,parameters,keep_prob=1)\n",
    "outTrain,cache=L_model_forward(Xtrain,parameters,keep_prob=1)\n",
    "outTest,cache=L_model_forward(Xtest,parameters,keep_prob=1)\n",
    "    \n",
    "predictCV=(outCV>MejorUmbral)*1\n",
    "predictCV=np.transpose(predictCV)\n",
    "scoreCV=mes.accuracy_score(Ycv,predictCV)\n",
    "    \n",
    "predictTrain=(outTrain>MejorUmbral)*1\n",
    "predictTrain=np.transpose(predictTrain)\n",
    "scoreTrain=mes.accuracy_score(Ytrain,predictTrain)\n",
    "    \n",
    "predictTest=(outTest>MejorUmbral)*1\n",
    "predictTest=np.transpose(predictTest)\n",
    "scoreTest=mes.accuracy_score(Ytest,predictTest)\n",
    "\n",
    "print('Resultados del mejor modelo en el set de entrenamiento: ')\n",
    "print('Porcentaje de aciertos: ', scoreTrain)\n",
    "print('\\n')\n",
    "print('Recall,precision y valor-F del modelo')\n",
    "print(classification_report(Ytrain,predictTrain,target_names=['partícula 1','partícula 2']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Resultados del mejor modelo en el set de validación: ')\n",
    "print('Porcentaje de aciertos: ', scoreCV)\n",
    "print('\\n')\n",
    "print('Recall,precision y valor-F del modelo')\n",
    "print(classification_report(Ycv,predictCV,target_names=['partícula 1','partícula 2']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Resultados del mejor modelo en el set de prueba: ')\n",
    "print('Porcentaje de aciertos: ', scoreTest)\n",
    "print('\\n')\n",
    "print('Recall,precision y valor-F del modelo')\n",
    "print(classification_report(Ytest,predictTest,target_names=['partícula 1','partícula 2']))\n",
    "print('\\n')\n",
    "print('Parámetros utilizados para el mejor modelo: ')\n",
    "print('Mejor tasa de aprendizaje: ',MejorAlpha)\n",
    "print('Mejor tasa de decaimiento: ',MejorDecayRate)\n",
    "print('Mejor Número de neuronas por capa: ',MejorNn)\n",
    "print('Mejor Número de capas: ',MejorNc)\n",
    "print('Mejor probabilidad de dejar neuronas vivas: ',Mejorkp)\n",
    "print('Mejor parámetro de regularización L2: ',MejorLambda)\n",
    "print('Mejor umbral de selección: ',MejorUmbral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-1.46063926e+00, -7.79054350e-02,  1.05079943e-01,\n",
       "          3.09920549e-01, -4.21001547e-01, -8.68471377e-01,\n",
       "          1.61703353e-01,  2.13581261e-01, -3.39872984e-01,\n",
       "         -1.05410288e-01,  1.79276641e-01,  6.13064485e-01,\n",
       "         -1.29524050e+00, -2.99586536e-01, -8.19653870e-01,\n",
       "         -8.95866570e-01, -2.50364877e-01, -7.74489074e-01,\n",
       "         -6.16813843e-01, -1.04165363e+00, -3.20455252e-01,\n",
       "         -6.63547638e-01, -1.17463227e+00,  3.22662190e-01,\n",
       "         -1.45137809e+00, -1.77147653e-01, -1.69531796e-01,\n",
       "          4.39591408e-01, -1.05286924e-01, -6.47295249e-01],\n",
       "        [-2.00450985e-02,  5.85362750e-02,  5.24970415e-02,\n",
       "          2.05459766e-01, -6.26774942e-01,  6.79788822e-01,\n",
       "          3.45778410e-01, -2.73148168e-01,  6.70640808e-01,\n",
       "         -8.73701829e-02,  3.05069636e-01, -5.02809745e-02,\n",
       "          2.16668534e-01, -5.71787040e-01, -6.85804725e-01,\n",
       "         -1.03034241e-01,  2.84772925e-01, -1.43774894e-01,\n",
       "          3.78648620e-02, -5.73206958e-01, -2.24229128e-01,\n",
       "          2.82588594e-01, -1.91858916e-02,  1.15408429e-01,\n",
       "         -1.44498273e+00,  1.25231366e+00,  2.08405401e-01,\n",
       "         -4.28206430e-01,  4.18731178e-01, -3.64870202e-01],\n",
       "        [-5.28749483e-01,  7.76898867e-02, -2.63348173e-01,\n",
       "         -7.17978518e-01, -4.86704356e-01, -6.31822541e-01,\n",
       "         -1.26782649e-01,  2.53350709e-01, -2.89480689e-01,\n",
       "          1.34610717e-01, -2.69134554e-01,  6.96117006e-01,\n",
       "         -6.58146773e-01, -3.85978163e-01, -1.05455314e+00,\n",
       "         -4.94706148e-01, -1.50073941e-01, -7.57757206e-01,\n",
       "         -1.64182210e-01, -1.47470747e+00, -1.32247700e-01,\n",
       "         -5.80796004e-01, -1.26578073e+00,  4.92466706e-01,\n",
       "         -1.78613619e+00,  7.22974054e-01, -1.98764122e-01,\n",
       "          9.00524633e-03, -3.74765794e-01, -5.59993329e-01],\n",
       "        [-4.46206925e-01,  4.94294239e-02, -1.99265673e-01,\n",
       "          1.06934917e-01, -8.34533788e-02, -2.83880016e-01,\n",
       "         -2.08582947e-01,  1.42865931e-01, -4.08385986e-02,\n",
       "          3.13689614e-01, -1.06407386e-01,  8.00817911e-01,\n",
       "         -1.88944927e-01, -6.09429224e-01, -3.14095133e-01,\n",
       "         -1.08028125e+00,  7.89476993e-02, -2.13783176e-01,\n",
       "          1.32015991e-01, -4.82667989e-01, -4.79121634e-02,\n",
       "         -4.16473110e-02, -3.56227640e-01,  6.13304378e-01,\n",
       "         -1.35597667e+00,  3.00153823e-01,  4.58273568e-01,\n",
       "         -3.00788133e-01,  2.67869064e-01, -8.36036566e-01],\n",
       "        [-5.03327841e-01, -4.61250073e-01,  1.21012066e-01,\n",
       "         -4.06752708e-02, -6.16840238e-01,  4.30999832e-01,\n",
       "          1.60923898e-01, -1.77680929e-01,  1.83787815e-01,\n",
       "         -2.30917685e-02,  8.97464805e-01, -2.47576804e-01,\n",
       "          2.79814068e-01, -3.48645095e-01, -4.02970313e-01,\n",
       "          2.18613785e-01,  6.23469196e-01, -1.68392976e-01,\n",
       "          2.21256209e-01, -5.92983289e-01, -3.00401135e-01,\n",
       "          1.22091036e-01, -5.69959303e-01, -6.32827138e-01,\n",
       "         -8.95257030e-01,  1.33971312e+00,  3.83217739e-01,\n",
       "          1.14128779e-01,  4.04848576e-01, -1.75058420e-01],\n",
       "        [-9.64791745e-02, -5.51764595e-01, -1.71365793e-01,\n",
       "          1.65343872e-01, -3.67836294e-01,  4.09865488e-01,\n",
       "          4.60781903e-01, -2.72210994e-01,  7.70887918e-02,\n",
       "         -3.25000224e-01,  3.61952365e-01,  1.07946039e-01,\n",
       "          1.17744178e-01, -2.05313643e-01, -2.28694224e-01,\n",
       "         -4.68057439e-02,  1.61879426e-01, -4.24581672e-01,\n",
       "          1.49378524e-01, -1.76405435e-01, -1.24662577e-01,\n",
       "          3.64510989e-02, -6.80816085e-01,  3.85114360e-01,\n",
       "         -5.12739126e-01,  9.39258815e-01,  1.43969532e-01,\n",
       "         -1.57803727e-02, -8.67813400e-02,  4.85905541e-01],\n",
       "        [-6.24505301e-01, -3.98401890e-01, -1.32356257e-01,\n",
       "         -3.53164886e-01, -4.83438234e-01,  5.29513131e-01,\n",
       "          6.00578664e-02,  3.41589266e-01, -6.46634490e-01,\n",
       "          3.64775292e-01,  1.68464542e-01,  6.63264760e-01,\n",
       "          5.42652183e-02, -3.04626341e-01, -2.38140174e-01,\n",
       "         -1.05297916e+00, -8.97342590e-01,  4.38042375e-01,\n",
       "         -5.91017175e-01, -3.85501254e-01, -3.42981634e-01,\n",
       "         -1.67646809e-01, -5.70461585e-01,  2.19921251e-01,\n",
       "         -1.15015325e+00,  1.09004749e+00,  5.78633269e-01,\n",
       "          4.59331041e-01, -5.34076901e-01, -6.50700325e-01],\n",
       "        [ 5.42462919e-01,  7.20959154e-02,  2.71278307e-01,\n",
       "          3.04388391e-01,  7.69251249e-02, -4.54764778e-01,\n",
       "          2.40467097e-01, -2.26850958e-01,  1.89307525e-01,\n",
       "         -1.35265022e-01, -1.28454078e+00, -3.75171146e-01,\n",
       "         -2.96486043e-01,  6.53237898e-01,  6.12697527e-01,\n",
       "          1.67746829e-01,  1.84209488e-01,  8.40238386e-02,\n",
       "          8.01383741e-02,  5.95840699e-01,  3.63340392e-01,\n",
       "         -1.23420972e-01,  1.48400019e-01, -4.60797198e-01,\n",
       "          8.53471771e-01, -1.87072555e+00, -8.13308593e-01,\n",
       "          3.04672781e-01, -1.69777465e-01,  3.22897548e-01],\n",
       "        [ 3.15401637e-01,  2.28559443e-01, -1.86129551e-04,\n",
       "         -2.48280366e-01, -6.05716932e-02, -6.81673072e-01,\n",
       "         -3.62599213e-02,  4.89198936e-01,  2.19505805e-02,\n",
       "         -5.73652089e-03, -1.16504167e+00,  4.26993083e-01,\n",
       "         -5.23435624e-01,  1.13061431e+00, -1.07231657e-01,\n",
       "         -1.69101073e-01, -1.79702780e-01,  3.43212517e-01,\n",
       "         -6.29516479e-02, -4.59375941e-02,  1.06362553e-01,\n",
       "         -3.02563721e-01,  5.13119842e-01,  4.72144215e-01,\n",
       "          1.14049957e+00, -1.26275854e+00,  2.07259655e-02,\n",
       "          1.84658689e-01,  7.16479258e-02,  3.67633825e-01],\n",
       "        [-4.68241920e-01,  6.96451539e-02, -1.37181879e-02,\n",
       "          6.39124659e-02, -6.04833886e-01,  8.19631747e-01,\n",
       "          3.41095373e-01, -2.07106942e-02,  1.02721756e-01,\n",
       "         -6.44996238e-03,  8.03953371e-01, -1.92163293e-01,\n",
       "          7.10639156e-01, -5.25298120e-01,  4.46614706e-01,\n",
       "         -2.22078886e-01,  3.04418044e-01, -3.50902488e-01,\n",
       "          2.22260818e-01, -1.80002983e-01, -3.26278227e-01,\n",
       "          3.22346526e-01, -4.74876861e-02, -3.44267835e-01,\n",
       "         -9.67208886e-01,  1.66603719e+00,  3.85997020e-01,\n",
       "         -1.61321176e-01, -4.69329260e-01, -5.11447638e-01],\n",
       "        [-1.93284898e-01, -1.04572216e-01,  1.17109806e-01,\n",
       "         -4.51171722e-02, -8.43381779e-01,  3.84102807e-01,\n",
       "         -2.73774747e-01,  2.67829013e-01, -2.05706756e-01,\n",
       "         -1.42136291e-01,  1.06780606e+00, -4.10727419e-02,\n",
       "          3.76471678e-01, -8.37219481e-01, -3.32837975e-01,\n",
       "         -3.76365174e-02,  1.41468710e-01, -2.78041108e-01,\n",
       "         -1.80292753e-01,  4.71165346e-01, -9.14014487e-02,\n",
       "          2.08425314e-01, -9.91750984e-02, -5.44943329e-01,\n",
       "         -1.11177439e+00,  1.50272141e+00,  4.11265837e-02,\n",
       "         -1.64061200e-01,  2.67369118e-01, -2.31755968e-01],\n",
       "        [-6.49197474e-01,  1.03741343e+00,  2.73075109e-02,\n",
       "          5.70801463e-01, -3.54103083e-01, -8.77033132e-01,\n",
       "          1.11714721e-01, -2.98531229e-01, -4.06670014e-02,\n",
       "         -5.33246346e-01, -1.31592217e-02, -9.99181255e-02,\n",
       "         -4.41198673e-01, -3.13263350e-02, -2.36932709e-01,\n",
       "          7.33611650e-01, -1.55724214e-01, -5.69555700e-01,\n",
       "          2.79776111e-01, -6.11907191e-01,  3.88534750e-01,\n",
       "         -5.11974069e-01, -3.14005446e-01,  2.82887084e-01,\n",
       "         -2.94405071e-02, -6.84370987e-01, -1.60157380e-01,\n",
       "         -7.80435362e-02,  5.95907240e-01,  3.63485934e-03],\n",
       "        [-7.97407326e-02,  1.04972728e-01, -1.49435456e-01,\n",
       "          2.32213388e-02, -6.36983959e-01,  6.58152322e-03,\n",
       "          5.24775471e-01, -2.45823679e-01,  4.33048680e-01,\n",
       "          9.49704509e-02,  6.74207832e-01, -5.88937865e-01,\n",
       "          4.55498329e-01, -6.40728888e-01, -2.69899655e-01,\n",
       "          7.46332829e-02, -2.82193619e-02,  1.15881170e-01,\n",
       "          1.34535601e-01, -3.79597736e-01, -6.11363805e-01,\n",
       "         -3.91694663e-01, -1.90333821e-01, -1.39044710e-01,\n",
       "         -6.89553101e-01,  2.05138943e+00,  1.95580946e-01,\n",
       "         -3.16999568e-02,  7.03888395e-02, -3.53200175e-01],\n",
       "        [-5.29154292e-01, -2.15993106e-01,  2.51496937e-02,\n",
       "          3.61809192e-01, -4.44587930e-01,  6.89994958e-01,\n",
       "         -1.14974737e-01,  1.72521229e-01,  1.48751866e-01,\n",
       "          1.09590036e-01,  6.98746146e-01, -1.70979368e-01,\n",
       "         -1.42836828e-02, -8.05197040e-01,  7.65277052e-02,\n",
       "         -6.69307344e-01, -2.18871518e-01,  1.86687653e-01,\n",
       "          5.74131204e-02, -6.89690921e-01, -1.50490799e-01,\n",
       "         -4.89802435e-01,  2.87586804e-02, -6.55391194e-02,\n",
       "         -1.12809947e+00,  1.58583820e+00,  4.86825427e-01,\n",
       "         -1.65264005e-01,  8.19520792e-02, -2.34133181e-01],\n",
       "        [-8.33139350e-01, -6.06756555e-01, -7.92439496e-01,\n",
       "         -8.60856951e-01, -4.20171535e-01, -1.90384126e-01,\n",
       "         -2.73669987e-01,  8.55182340e-01, -5.70508662e-01,\n",
       "          7.31260663e-01,  2.62455270e-01,  5.06068465e-01,\n",
       "         -3.60271365e-01, -8.44352070e-01, -1.07238674e+00,\n",
       "         -5.94086408e-01, -4.27397417e-01, -1.98860779e-01,\n",
       "         -1.93294818e-01, -9.77256995e-01, -5.51472398e-01,\n",
       "         -1.97789308e-01, -9.47413998e-01,  4.55432729e-01,\n",
       "         -2.04265609e+00,  8.82448264e-01,  9.67749423e-01,\n",
       "         -3.24751450e-01, -3.38127953e-01, -6.27894618e-01],\n",
       "        [ 7.69335374e-03,  4.69470514e-03,  5.18385527e-01,\n",
       "         -2.71414445e-01, -2.18926664e-01, -8.36199580e-02,\n",
       "          2.93184537e-02,  5.49354800e-02,  1.50236631e-01,\n",
       "         -2.83364948e-02,  7.44245881e-02,  5.26713387e-02,\n",
       "          5.62828738e-01, -1.83818730e-01,  1.45886513e-01,\n",
       "          6.54056339e-01, -1.51611619e-01, -4.53310299e-01,\n",
       "         -4.94568072e-02, -2.16243253e-03,  7.88974445e-03,\n",
       "          2.97780465e-01, -4.04483750e-01, -2.58794167e-01,\n",
       "         -5.68727722e-01,  7.68730301e-01,  1.21328487e-01,\n",
       "         -1.85493261e-01, -8.49458720e-02,  6.16436248e-01],\n",
       "        [-8.27882300e-01,  5.86103649e-02,  4.24518067e-02,\n",
       "         -3.64343634e-02,  1.26074679e-01, -8.11896518e-01,\n",
       "         -6.97307978e-01, -2.19089769e-01, -1.78243949e-01,\n",
       "          5.09464407e-01, -6.02564785e-01,  2.44992177e-01,\n",
       "         -2.23382886e-01, -5.63641703e-01, -1.08577379e+00,\n",
       "         -9.77256446e-01, -3.57229547e-02, -5.31828775e-01,\n",
       "         -3.43417678e-01, -7.42886320e-01, -5.76532166e-01,\n",
       "         -4.83925703e-01, -7.24785283e-01,  5.32375509e-01,\n",
       "         -1.27065491e+00,  1.04787034e-01,  7.51038988e-02,\n",
       "         -3.96851706e-01, -2.30126570e-01, -9.21837962e-01],\n",
       "        [-9.34785867e-01, -4.09354015e-01, -1.10624357e-01,\n",
       "         -6.33685531e-01, -1.03787954e+00, -3.59201849e-01,\n",
       "          8.95278449e-02,  6.84737690e-01, -9.51622017e-01,\n",
       "          5.13401656e-01, -7.52781335e-01,  1.68148945e-01,\n",
       "         -1.08542512e+00, -4.60300579e-01, -6.33440996e-01,\n",
       "         -3.76952845e-01, -7.44738120e-01, -2.53648161e-01,\n",
       "         -6.37330840e-01, -7.50847986e-01, -5.18532388e-01,\n",
       "         -3.93895512e-01, -1.46164512e-01,  6.66577084e-01,\n",
       "          1.94201954e-01, -6.58593599e-01, -5.53127643e-03,\n",
       "          8.91643407e-01, -5.62580474e-01,  2.44878869e-02],\n",
       "        [-2.25926400e-02, -2.72285438e-02,  6.76434577e-02,\n",
       "         -2.48446406e-02, -1.38598788e-01, -1.47073854e+00,\n",
       "         -2.44509237e-01, -5.37578936e-01, -1.83305743e-01,\n",
       "          3.14768910e-01, -7.95834471e-01,  7.30703905e-01,\n",
       "         -9.30174237e-01, -2.63609847e-01, -4.93500824e-01,\n",
       "          7.57393855e-02, -1.69101356e-01, -4.88392053e-01,\n",
       "         -2.36855284e-01, -5.19772488e-01,  3.68921319e-01,\n",
       "         -8.16749457e-01, -4.89941723e-01,  9.67935039e-01,\n",
       "          3.22019687e-01, -1.44340247e+00,  2.40296514e-01,\n",
       "         -5.94440183e-01,  2.94537709e-01, -7.58877387e-02],\n",
       "        [ 1.16252156e-01, -5.53666768e-01, -6.00616117e-01,\n",
       "          5.14695733e-01,  1.12703369e-01, -1.43617198e+00,\n",
       "         -4.12790489e-01,  3.27829718e-01, -2.07319395e-01,\n",
       "          2.31072734e-01, -8.97996315e-01,  4.21008646e-02,\n",
       "         -6.78593349e-02,  8.79463059e-01,  1.98044423e-01,\n",
       "         -3.49157897e-01,  3.71927189e-01,  2.23409375e-01,\n",
       "          1.88412512e-01,  5.02255551e-01,  4.98236045e-01,\n",
       "         -2.13021991e-01,  3.11930388e-01, -3.80760907e-02,\n",
       "          6.22606380e-01, -2.02723846e+00, -1.39643288e-01,\n",
       "          2.75639615e-01, -1.41391092e-01,  9.79418816e-02],\n",
       "        [-4.83342082e-01, -1.27439495e-01, -3.14346385e-01,\n",
       "          4.14178322e-01, -4.33069452e-01, -4.04361566e-01,\n",
       "          6.45938215e-01, -2.59378009e-01, -5.45491061e-01,\n",
       "          4.03721575e-01, -1.46367547e-01,  5.89626231e-01,\n",
       "         -9.44930448e-01, -4.65002350e-02, -5.73762686e-01,\n",
       "         -1.62826888e-01, -4.37432952e-01, -5.86624963e-01,\n",
       "          4.96482647e-02, -9.42823475e-01, -1.58233696e-01,\n",
       "         -9.74744031e-01, -8.19899881e-01,  5.68773508e-02,\n",
       "         -4.61538610e-01, -7.06045648e-01,  3.01754769e-01,\n",
       "         -5.74067648e-01, -2.18410292e-01, -8.67054929e-01],\n",
       "        [ 4.82943399e-02, -3.35802948e-01, -2.65725616e-01,\n",
       "         -8.33125305e-02, -7.68411814e-01, -1.95107875e-01,\n",
       "         -4.73396543e-01,  9.42399124e-01, -2.23153841e-01,\n",
       "          5.70991128e-01, -2.88895135e-01,  3.50452196e-01,\n",
       "         -2.50570006e-01, -8.20021560e-01, -4.68033361e-01,\n",
       "         -3.98051902e-01, -5.19771792e-01, -4.62540742e-01,\n",
       "          1.18558050e-02, -4.51897550e-01, -6.21381841e-01,\n",
       "         -1.10577038e+00, -4.29580399e-01,  1.00642102e+00,\n",
       "         -3.74872611e-02, -1.03915692e+00,  2.57609230e-01,\n",
       "          2.85001638e-01, -7.30864581e-01, -1.31281301e-01],\n",
       "        [-3.43440773e-01, -3.02121267e-01,  1.66807660e-01,\n",
       "         -3.23174852e-01, -1.05733652e-01,  9.85037584e-01,\n",
       "          3.97839100e-01,  1.98483168e-01,  3.40785370e-01,\n",
       "          9.27676722e-03,  9.32278836e-01, -1.95210645e-01,\n",
       "          4.87668373e-01, -5.31633493e-01, -2.09856162e-01,\n",
       "         -1.27347111e-01,  2.01021452e-01,  6.31356036e-01,\n",
       "          3.84103784e-01, -2.04206823e-01,  3.10935318e-01,\n",
       "         -9.17684309e-02, -5.75231568e-01, -5.18909508e-01,\n",
       "         -1.14550718e+00,  1.99332076e+00,  9.13247086e-02,\n",
       "         -9.67687638e-03, -2.23600734e-01, -4.88111507e-01],\n",
       "        [ 3.07956281e-01,  2.76481378e-01,  3.59979719e-01,\n",
       "         -5.10489058e-02, -4.47008213e-01,  7.09043565e-01,\n",
       "          2.32025225e-01,  1.10637782e-01, -2.61014949e-01,\n",
       "         -1.92319398e-01,  2.17874651e-01, -2.27005628e-01,\n",
       "         -3.14530231e-02, -7.23430812e-01,  2.21329384e-01,\n",
       "          5.82330704e-02, -2.08258885e-01, -2.09269865e-01,\n",
       "         -3.44759594e-01, -6.42760663e-02,  2.86287963e-01,\n",
       "         -2.46731230e-01, -2.84144534e-01, -2.70926224e-01,\n",
       "         -3.14652974e-01,  7.72950276e-01,  1.70780927e-01,\n",
       "         -2.64850755e-02, -1.44209868e-01,  8.12516045e-02],\n",
       "        [-1.05596777e+00, -3.25505958e-01, -5.31588340e-01,\n",
       "         -4.28367141e-01, -8.37789173e-01, -4.54640144e-02,\n",
       "         -5.83108846e-02,  3.09894664e-01, -4.68451128e-01,\n",
       "          4.96672701e-01, -1.66065385e-01,  5.43547929e-01,\n",
       "         -7.89347578e-01, -8.14343044e-01, -6.68984586e-01,\n",
       "          1.31063422e-01, -4.17691518e-01, -7.81181594e-01,\n",
       "         -4.75703255e-01, -7.98661410e-01, -6.86835103e-01,\n",
       "         -6.50681071e-01, -1.16812458e+00,  7.22022490e-01,\n",
       "         -1.06982819e+00,  5.91596626e-01,  1.15125866e-01,\n",
       "          2.19864737e-01, -4.63577643e-01,  2.94039354e-02],\n",
       "        [-5.46366338e-01,  5.76996689e-01,  5.97506247e-02,\n",
       "          4.46964263e-01,  2.39120264e-01, -6.59276675e-01,\n",
       "          9.97665803e-02, -4.84366182e-01,  3.75450545e-01,\n",
       "          6.46718221e-01, -1.03061262e+00,  7.07876687e-02,\n",
       "         -6.67577226e-01,  1.69909035e-01, -1.10845199e-01,\n",
       "          1.01346436e-01, -5.02666170e-01, -1.41069310e-01,\n",
       "          1.08008282e-01, -1.26790949e-02,  3.65696944e-01,\n",
       "         -2.43759359e-01, -3.58456277e-01,  4.79910381e-01,\n",
       "          3.01586878e-01, -1.75805063e+00, -3.20953384e-01,\n",
       "          2.53126781e-01,  3.51677491e-01, -5.27243858e-01]]),\n",
       " 'b1': array([[-1.35571633],\n",
       "        [-0.35640919],\n",
       "        [-1.06405497],\n",
       "        [-0.51330873],\n",
       "        [-0.23069404],\n",
       "        [-0.2715354 ],\n",
       "        [-0.6607509 ],\n",
       "        [ 0.23046518],\n",
       "        [ 0.23006027],\n",
       "        [-0.23030184],\n",
       "        [-0.35640653],\n",
       "        [-0.82989937],\n",
       "        [-0.23033897],\n",
       "        [-0.35648913],\n",
       "        [-0.92285036],\n",
       "        [-0.18618957],\n",
       "        [-0.74703929],\n",
       "        [-0.87606122],\n",
       "        [-0.90559103],\n",
       "        [ 0.23042109],\n",
       "        [-0.42910609],\n",
       "        [-0.37538749],\n",
       "        [-0.23027468],\n",
       "        [-0.2841131 ],\n",
       "        [-0.91658066],\n",
       "        [-0.20666738]]),\n",
       " 'W2': array([[ 1.21591699e-02, -2.78076461e-02,  1.37726085e-03,\n",
       "         -2.33598118e-01, -3.99468690e-02,  1.92561797e-02,\n",
       "          4.88520093e-02,  6.84369953e-02,  2.77407369e-01,\n",
       "         -1.13259625e-01, -1.99248663e-01,  4.00577976e-02,\n",
       "         -1.08485957e-01, -4.28700464e-02, -9.37431150e-03,\n",
       "          6.31729156e-02, -7.65381318e-02,  6.27585200e-02,\n",
       "          5.15364477e-02,  1.08554597e-01, -6.02053948e-02,\n",
       "         -1.02123466e-02, -1.50460205e-01,  9.70346330e-05,\n",
       "          1.94976572e-02, -2.22561539e-02]]),\n",
       " 'b2': array([[0.22987829]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parámetros \"óptimos\" de la red manual:\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicción con XtestT (creación del archivo ytest con las predicciones)\n",
    "XtestT=np.transpose(np.array(XtestT))\n",
    "outTestT,cache=L_model_forward(XtestT,parameters,keep_prob=1)\n",
    "predictTestT=(outTestT>MejorUmbral)*1\n",
    "predictTestT=pd.DataFrame(np.transpose(predictTestT))\n",
    "predictTestT.to_excel('ytest.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación con librería SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75796242\n",
      "Iteration 2, loss = 0.68301056\n",
      "Iteration 3, loss = 0.66264614\n",
      "Iteration 4, loss = 0.64984066\n",
      "Iteration 5, loss = 0.64086046\n",
      "Iteration 6, loss = 0.63413037\n",
      "Iteration 7, loss = 0.62867687\n",
      "Iteration 8, loss = 0.62470393\n",
      "Iteration 9, loss = 0.62102590\n",
      "Iteration 10, loss = 0.61787459\n",
      "Iteration 11, loss = 0.61508038\n",
      "Iteration 12, loss = 0.61254959\n",
      "Iteration 13, loss = 0.61020213\n",
      "Iteration 14, loss = 0.60792265\n",
      "Iteration 15, loss = 0.60540479\n",
      "Iteration 16, loss = 0.60349454\n",
      "Iteration 17, loss = 0.60184643\n",
      "Iteration 18, loss = 0.60010549\n",
      "Iteration 19, loss = 0.59883477\n",
      "Iteration 20, loss = 0.59654045\n",
      "Iteration 21, loss = 0.59527094\n",
      "Iteration 22, loss = 0.59400921\n",
      "Iteration 23, loss = 0.59306216\n",
      "Iteration 24, loss = 0.59119980\n",
      "Iteration 25, loss = 0.58954008\n",
      "Iteration 26, loss = 0.58878767\n",
      "Iteration 27, loss = 0.58733003\n",
      "Iteration 28, loss = 0.58670329\n",
      "Iteration 29, loss = 0.58565395\n",
      "Iteration 30, loss = 0.58472386\n",
      "Iteration 31, loss = 0.58389803\n",
      "Iteration 32, loss = 0.58279131\n",
      "Iteration 33, loss = 0.58257583\n",
      "Iteration 34, loss = 0.58142980\n",
      "Iteration 35, loss = 0.58124324\n",
      "Iteration 36, loss = 0.58005291\n",
      "Iteration 37, loss = 0.57939955\n",
      "Iteration 38, loss = 0.57820378\n",
      "Iteration 39, loss = 0.57789094\n",
      "Iteration 40, loss = 0.57717892\n",
      "Iteration 41, loss = 0.57618341\n",
      "Iteration 42, loss = 0.57591039\n",
      "Iteration 43, loss = 0.57521971\n",
      "Iteration 44, loss = 0.57484393\n",
      "Iteration 45, loss = 0.57409087\n",
      "Iteration 46, loss = 0.57342100\n",
      "Iteration 47, loss = 0.57271327\n",
      "Iteration 48, loss = 0.57254456\n",
      "Iteration 49, loss = 0.57205555\n",
      "Iteration 50, loss = 0.57154539\n",
      "Iteration 51, loss = 0.57044119\n",
      "Iteration 52, loss = 0.57049364\n",
      "Iteration 53, loss = 0.56914211\n",
      "Iteration 54, loss = 0.56879550\n",
      "Iteration 55, loss = 0.56812151\n",
      "Iteration 56, loss = 0.56758701\n",
      "Iteration 57, loss = 0.56768884\n",
      "Iteration 58, loss = 0.56723643\n",
      "Iteration 59, loss = 0.56686039\n",
      "Iteration 60, loss = 0.56641474\n",
      "Iteration 61, loss = 0.56587011\n",
      "Iteration 62, loss = 0.56522484\n",
      "Iteration 63, loss = 0.56534038\n",
      "Iteration 64, loss = 0.56462909\n",
      "Iteration 65, loss = 0.56423954\n",
      "Iteration 66, loss = 0.56425976\n",
      "Iteration 67, loss = 0.56402085\n",
      "Iteration 68, loss = 0.56322141\n",
      "Iteration 69, loss = 0.56312958\n",
      "Iteration 70, loss = 0.56233857\n",
      "Iteration 71, loss = 0.56264713\n",
      "Iteration 72, loss = 0.56232214\n",
      "Iteration 73, loss = 0.56219118\n",
      "Iteration 74, loss = 0.56120027\n",
      "Iteration 75, loss = 0.56171610\n",
      "Iteration 76, loss = 0.56062038\n",
      "Iteration 77, loss = 0.56070388\n",
      "Iteration 78, loss = 0.56016790\n",
      "Iteration 79, loss = 0.55991564\n",
      "Iteration 80, loss = 0.56071881\n",
      "Iteration 81, loss = 0.55965266\n",
      "Iteration 82, loss = 0.55906404\n",
      "Iteration 83, loss = 0.55879266\n",
      "Iteration 84, loss = 0.55884276\n",
      "Iteration 85, loss = 0.55854894\n",
      "Iteration 86, loss = 0.55828607\n",
      "Iteration 87, loss = 0.55723673\n",
      "Iteration 88, loss = 0.55837197\n",
      "Iteration 89, loss = 0.55732380\n",
      "Iteration 90, loss = 0.55738810\n",
      "Iteration 91, loss = 0.55687594\n",
      "Iteration 92, loss = 0.55679495\n",
      "Iteration 93, loss = 0.55611162\n",
      "Iteration 94, loss = 0.55587929\n",
      "Iteration 95, loss = 0.55545118\n",
      "Iteration 96, loss = 0.55585101\n",
      "Iteration 97, loss = 0.55530602\n",
      "Iteration 98, loss = 0.55485077\n",
      "Iteration 99, loss = 0.55450803\n",
      "Iteration 100, loss = 0.55451380\n",
      "Iteration 101, loss = 0.55435458\n",
      "Iteration 102, loss = 0.55403858\n",
      "Iteration 103, loss = 0.55403799\n",
      "Iteration 104, loss = 0.55361472\n",
      "Iteration 105, loss = 0.55386882\n",
      "Iteration 106, loss = 0.55310725\n",
      "Iteration 107, loss = 0.55268578\n",
      "Iteration 108, loss = 0.55305753\n",
      "Iteration 109, loss = 0.55310051\n",
      "Iteration 110, loss = 0.55220254\n",
      "Iteration 111, loss = 0.55248559\n",
      "Iteration 112, loss = 0.55218717\n",
      "Iteration 113, loss = 0.55189156\n",
      "Iteration 114, loss = 0.55134621\n",
      "Iteration 115, loss = 0.55169119\n",
      "Iteration 116, loss = 0.55128142\n",
      "Iteration 117, loss = 0.55135287\n",
      "Iteration 118, loss = 0.55035652\n",
      "Iteration 119, loss = 0.55046645\n",
      "Iteration 120, loss = 0.55032095\n",
      "Iteration 121, loss = 0.54998536\n",
      "Iteration 122, loss = 0.55036437\n",
      "Iteration 123, loss = 0.54954948\n",
      "Iteration 124, loss = 0.54964462\n",
      "Iteration 125, loss = 0.54924064\n",
      "Iteration 126, loss = 0.54998555\n",
      "Iteration 127, loss = 0.54902321\n",
      "Iteration 128, loss = 0.54891419\n",
      "Iteration 129, loss = 0.54849003\n",
      "Iteration 130, loss = 0.54844152\n",
      "Iteration 131, loss = 0.54854561\n",
      "Iteration 132, loss = 0.54845330\n",
      "Iteration 133, loss = 0.54849544\n",
      "Iteration 134, loss = 0.54824554\n",
      "Iteration 135, loss = 0.54824029\n",
      "Iteration 136, loss = 0.54749223\n",
      "Iteration 137, loss = 0.54761053\n",
      "Iteration 138, loss = 0.54790277\n",
      "Iteration 139, loss = 0.54702611\n",
      "Iteration 140, loss = 0.54731505\n",
      "Iteration 141, loss = 0.54655302\n",
      "Iteration 142, loss = 0.54688507\n",
      "Iteration 143, loss = 0.54700538\n",
      "Iteration 144, loss = 0.54682168\n",
      "Iteration 145, loss = 0.54677181\n",
      "Iteration 146, loss = 0.54635534\n",
      "Iteration 147, loss = 0.54613490\n",
      "Iteration 148, loss = 0.54579154\n",
      "Iteration 149, loss = 0.54567713\n",
      "Iteration 150, loss = 0.54588499\n",
      "Iteration 151, loss = 0.54592343\n",
      "Iteration 152, loss = 0.54569492\n",
      "Iteration 153, loss = 0.54552657\n",
      "Iteration 154, loss = 0.54541472\n",
      "Iteration 155, loss = 0.54467761\n",
      "Iteration 156, loss = 0.54481905\n",
      "Iteration 157, loss = 0.54505327\n",
      "Iteration 158, loss = 0.54481486\n",
      "Iteration 159, loss = 0.54426160\n",
      "Iteration 160, loss = 0.54403141\n",
      "Iteration 161, loss = 0.54392271\n",
      "Iteration 162, loss = 0.54437406\n",
      "Iteration 163, loss = 0.54400816\n",
      "Iteration 164, loss = 0.54350361\n",
      "Iteration 165, loss = 0.54440944\n",
      "Iteration 166, loss = 0.54311302\n",
      "Iteration 167, loss = 0.54389918\n",
      "Iteration 168, loss = 0.54342196\n",
      "Iteration 169, loss = 0.54326644\n",
      "Iteration 170, loss = 0.54282870\n",
      "Iteration 171, loss = 0.54312143\n",
      "Iteration 172, loss = 0.54319738\n",
      "Iteration 173, loss = 0.54341068\n",
      "Iteration 174, loss = 0.54258861\n",
      "Iteration 175, loss = 0.54316240\n",
      "Iteration 176, loss = 0.54277178\n",
      "Iteration 177, loss = 0.54237762\n",
      "Iteration 178, loss = 0.54237884\n",
      "Iteration 179, loss = 0.54292760\n",
      "Iteration 180, loss = 0.54265993\n",
      "Iteration 181, loss = 0.54191838\n",
      "Iteration 182, loss = 0.54181967\n",
      "Iteration 183, loss = 0.54178258\n",
      "Iteration 184, loss = 0.54226175\n",
      "Iteration 185, loss = 0.54165348\n",
      "Iteration 186, loss = 0.54105823\n",
      "Iteration 187, loss = 0.54104696\n",
      "Iteration 188, loss = 0.54130026\n",
      "Iteration 189, loss = 0.54117385\n",
      "Iteration 190, loss = 0.54107095\n",
      "Iteration 191, loss = 0.54093099\n",
      "Iteration 192, loss = 0.54108062\n",
      "Iteration 193, loss = 0.54096878\n",
      "Iteration 194, loss = 0.54069977\n",
      "Iteration 195, loss = 0.54051362\n",
      "Iteration 196, loss = 0.54045671\n",
      "Iteration 197, loss = 0.54120228\n",
      "Iteration 198, loss = 0.54031297\n",
      "Iteration 199, loss = 0.53933344\n",
      "Iteration 200, loss = 0.53959872\n",
      "Iteration 201, loss = 0.53976354\n",
      "Iteration 202, loss = 0.53951339\n",
      "Iteration 203, loss = 0.53991502\n",
      "Iteration 204, loss = 0.53935064\n",
      "Iteration 205, loss = 0.53933168\n",
      "Iteration 206, loss = 0.53962059\n",
      "Iteration 207, loss = 0.54036106\n",
      "Iteration 208, loss = 0.53897107\n",
      "Iteration 209, loss = 0.53926368\n",
      "Iteration 210, loss = 0.53924013\n",
      "Iteration 211, loss = 0.53965398\n",
      "Iteration 212, loss = 0.53937021\n",
      "Iteration 213, loss = 0.53887834\n",
      "Iteration 214, loss = 0.53965723\n",
      "Iteration 215, loss = 0.53872275\n",
      "Iteration 216, loss = 0.53887535\n",
      "Iteration 217, loss = 0.53898507\n",
      "Iteration 218, loss = 0.53877430\n",
      "Iteration 219, loss = 0.53845992\n",
      "Iteration 220, loss = 0.53862730\n",
      "Iteration 221, loss = 0.53841672\n",
      "Iteration 222, loss = 0.53828711\n",
      "Iteration 223, loss = 0.53854179\n",
      "Iteration 224, loss = 0.53831575\n",
      "Iteration 225, loss = 0.53844152\n",
      "Iteration 226, loss = 0.53773360\n",
      "Iteration 227, loss = 0.53775590\n",
      "Iteration 228, loss = 0.53844616\n",
      "Iteration 229, loss = 0.53735038\n",
      "Iteration 230, loss = 0.53815197\n",
      "Iteration 231, loss = 0.53782748\n",
      "Iteration 232, loss = 0.53746762\n",
      "Iteration 233, loss = 0.53819959\n",
      "Iteration 234, loss = 0.53777740\n",
      "Iteration 235, loss = 0.53748777\n",
      "Iteration 236, loss = 0.53826694\n",
      "Iteration 237, loss = 0.53819453\n",
      "Iteration 238, loss = 0.53753669\n",
      "Iteration 239, loss = 0.53738147\n",
      "Iteration 240, loss = 0.53700775\n",
      "Iteration 241, loss = 0.53765105\n",
      "Iteration 242, loss = 0.53793096\n",
      "Iteration 243, loss = 0.53694898\n",
      "Iteration 244, loss = 0.53786850\n",
      "Iteration 245, loss = 0.53711258\n",
      "Iteration 246, loss = 0.53739780\n",
      "Iteration 247, loss = 0.53770773\n",
      "Iteration 248, loss = 0.53722383\n",
      "Iteration 249, loss = 0.53702887\n",
      "Iteration 250, loss = 0.53763857\n",
      "Iteration 251, loss = 0.53646896\n",
      "Iteration 252, loss = 0.53702053\n",
      "Iteration 253, loss = 0.53673621\n",
      "Iteration 254, loss = 0.53698000\n",
      "Iteration 255, loss = 0.53665413\n",
      "Iteration 256, loss = 0.53716865\n",
      "Iteration 257, loss = 0.53646292\n",
      "Iteration 258, loss = 0.53651480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259, loss = 0.53684403\n",
      "Iteration 260, loss = 0.53654405\n",
      "Iteration 261, loss = 0.53672258\n",
      "Iteration 262, loss = 0.53659638\n",
      "Iteration 263, loss = 0.53628118\n",
      "Iteration 264, loss = 0.53607026\n",
      "Iteration 265, loss = 0.53610431\n",
      "Iteration 266, loss = 0.53682880\n",
      "Iteration 267, loss = 0.53644992\n",
      "Iteration 268, loss = 0.53552866\n",
      "Iteration 269, loss = 0.53631371\n",
      "Iteration 270, loss = 0.53688260\n",
      "Iteration 271, loss = 0.53627587\n",
      "Iteration 272, loss = 0.53591841\n",
      "Iteration 273, loss = 0.53598811\n",
      "Iteration 274, loss = 0.53558044\n",
      "Iteration 275, loss = 0.53658361\n",
      "Iteration 276, loss = 0.53653203\n",
      "Iteration 277, loss = 0.53609789\n",
      "Iteration 278, loss = 0.53592814\n",
      "Iteration 279, loss = 0.53570479\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Porcentaje de aciertos en el set de entrenamiento:  0.7290557215222756\n",
      "Porcentaje de aciertos en el set de validación:  0.66\n"
     ]
    }
   ],
   "source": [
    "#sklearn\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "classifier=MLP(hidden_layer_sizes=(16,),verbose=True,activation='relu',solver='adam',max_iter=800,alpha=0.001,learning_rate='constant', batch_size=64,tol=0.000001,validation_fraction=0)\n",
    "classifier.fit(Xtrainw,np.array(np.transpose(Ytrainw))[0])\n",
    "scoreTrain=classifier.score(Xtrainw,Ytrainw)\n",
    "scoreCV=classifier.score(Xcvw,Ycvw)\n",
    "print('Porcentaje de aciertos en el set de entrenamiento: ',scoreTrain)\n",
    "print('Porcentaje de aciertos en el set de validación: ',scoreCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.08723310e-01,  9.59453462e-02, -6.88750852e-01,\n",
       "         -4.52152628e-01, -1.09153758e-01, -2.11741195e-01,\n",
       "          1.16145199e-01, -1.75228276e-01,  3.12157580e-01,\n",
       "         -7.77740101e-01, -9.69680970e-01],\n",
       "        [ 1.40606987e-01, -2.52052976e-01, -2.16152640e-01,\n",
       "          8.50272572e-02,  2.53888693e-01, -1.90328188e-01,\n",
       "         -4.15900922e-01,  1.34048206e-01,  2.72393175e-01,\n",
       "          1.58776928e-01, -8.20009719e-02],\n",
       "        [-4.11732053e-02,  2.67236955e-01, -4.16717439e-01,\n",
       "          2.15490842e-01,  2.20329976e-02, -2.32998600e-01,\n",
       "          8.54999556e-01,  3.67200088e-01,  4.76975803e-01,\n",
       "          4.75100744e-01, -8.51424015e-01],\n",
       "        [-5.06866449e-02, -7.77892505e-02, -3.39021548e-01,\n",
       "          4.59124358e-02,  5.23593129e-03, -3.15503743e-01,\n",
       "         -2.29198992e-01,  2.87291870e-01,  1.26585744e-01,\n",
       "         -1.68415217e-01,  3.03965043e-01],\n",
       "        [ 2.08060044e-01, -8.25061927e-02, -3.94810222e-01,\n",
       "          7.95575340e-03, -2.55446569e-02, -3.72910395e-01,\n",
       "          7.98790327e-01, -4.09205415e-01, -3.84358557e-01,\n",
       "         -4.12594484e-01, -9.35140529e-01],\n",
       "        [ 1.47428621e-01, -2.28236480e-01, -3.00667939e-01,\n",
       "          3.63535551e-02,  1.40124059e-01, -6.45415171e-01,\n",
       "          8.15003389e-01,  1.48249703e-03, -1.31260688e-01,\n",
       "          2.30922789e-03,  3.74310665e-01],\n",
       "        [ 9.55519981e-02, -2.09632232e-01, -2.70509964e-01,\n",
       "          1.73429416e-01, -3.13894198e-01,  2.98484455e-01,\n",
       "         -2.09687975e-01, -5.13051442e-02, -5.73833833e-01,\n",
       "          8.13264056e-03,  1.17579076e-01],\n",
       "        [-6.46667150e-02, -1.41789467e-02, -4.91662445e-03,\n",
       "         -5.54722499e-02,  9.62664188e-02, -4.21225901e-02,\n",
       "          2.09928473e-01, -1.28842727e-01, -1.08381175e-01,\n",
       "          9.00668970e-03,  4.78408795e-02],\n",
       "        [-8.83809358e-02,  3.02995106e-01, -1.22677571e-01,\n",
       "         -3.46085472e-02, -3.87720535e-02, -2.14364697e-02,\n",
       "          1.39502981e-01,  3.60100749e-01,  2.43870521e-02,\n",
       "         -1.49105129e-01,  1.83619857e-01],\n",
       "        [-8.39986201e-02, -1.14825148e-01, -7.86223120e-02,\n",
       "         -4.99545023e-02,  5.19508883e-02,  3.42515291e-01,\n",
       "         -9.30462235e-03, -3.27362612e-01, -7.64325279e-02,\n",
       "          2.67391473e-01, -2.03074057e-02],\n",
       "        [ 1.01127144e+00, -1.29121782e+00,  2.72291644e-01,\n",
       "         -1.64605409e+00, -5.38404088e-01, -6.81909505e-01,\n",
       "          1.54870871e-01,  5.41542681e-02, -8.96062256e-03,\n",
       "          6.24950617e-02,  4.66420443e-01],\n",
       "        [ 2.57903776e-02, -4.31888472e-01, -1.71286472e-01,\n",
       "         -4.29940749e-01, -4.86253411e-02, -2.14692706e-01,\n",
       "         -3.78839905e-02, -9.67410897e-02, -1.36336802e-01,\n",
       "         -2.14490743e-01, -6.11811190e-02],\n",
       "        [-1.92424907e-01,  2.15958115e-01,  3.79078945e-01,\n",
       "         -6.20609729e-02, -2.34432150e-02, -2.07654388e-01,\n",
       "          1.36045559e-01,  4.63864777e-01,  2.88165880e-01,\n",
       "          6.66531535e-01,  4.95507819e-01],\n",
       "        [-9.46407094e-02, -1.44938399e-01,  1.31130780e-03,\n",
       "         -7.09856659e-02,  5.08818072e-01,  5.69072979e-01,\n",
       "         -3.41996822e-01,  5.98100006e-01,  9.04660570e-01,\n",
       "          4.16266872e-01, -4.00213180e-01],\n",
       "        [-5.39825377e-02,  5.75743157e-02,  5.25100942e-01,\n",
       "         -2.18691774e-01,  4.49552956e-01,  3.33521419e-01,\n",
       "         -1.92856116e-02, -2.59411175e-01,  3.55947207e-01,\n",
       "         -3.87054964e-01,  4.14591370e-01],\n",
       "        [ 9.01855001e-02, -7.58320757e-02, -2.31484944e-01,\n",
       "          2.50589617e-02, -2.40115684e-01, -3.81476359e-01,\n",
       "         -3.44234895e-02,  3.08154762e-01, -1.82530614e-01,\n",
       "         -3.83366191e-01, -4.70415163e-01],\n",
       "        [ 2.46232266e-01,  4.47453234e-02, -1.86826091e-01,\n",
       "         -4.56298784e-01, -9.65407503e-02,  6.55002288e-01,\n",
       "         -1.28124414e-01, -1.76080711e-01, -2.06447190e-01,\n",
       "          2.79062110e-01, -5.05623015e-02],\n",
       "        [-3.12371098e-01,  1.55186964e-01, -2.90217055e-01,\n",
       "         -7.12991105e-01,  3.72456332e-01, -7.08549933e-01,\n",
       "         -4.93927321e-02,  2.12094539e-01,  3.74372457e-01,\n",
       "          1.27834738e-01, -6.44125224e-02],\n",
       "        [ 5.58771393e-01,  8.86881830e-02, -2.37290632e-02,\n",
       "          2.79770753e-01,  4.57778653e-02,  9.78223502e-02,\n",
       "         -3.61192797e-01,  5.21100684e-01,  4.00128284e-01,\n",
       "          1.92982031e-01, -7.26158839e-02],\n",
       "        [ 2.47409845e-02,  3.89595356e-01,  2.38088869e-01,\n",
       "          5.24280124e-03,  3.17530563e-02,  2.65912981e-01,\n",
       "          1.22289052e-01,  2.25619106e-01, -2.08401777e-02,\n",
       "          6.34710313e-02,  1.32897476e-01],\n",
       "        [-1.10221594e-01, -5.16176332e-01,  3.16723493e-01,\n",
       "          2.29714661e-02,  2.53615684e-01,  1.32909130e-01,\n",
       "         -3.20242727e-01, -6.17102560e-01, -5.33564233e-01,\n",
       "          3.68496854e-01,  5.54362774e-02],\n",
       "        [ 8.02348010e-03, -1.20213863e-01,  3.96129983e-01,\n",
       "          1.97729727e-01, -1.58523266e-01,  6.36930494e-02,\n",
       "          4.64648232e-02, -1.26097044e-01, -3.04488944e-01,\n",
       "          1.52069476e-02,  6.74715568e-02],\n",
       "        [ 1.45662903e-01, -1.60833795e-01,  2.56362632e-01,\n",
       "          1.26870066e-01,  3.43055101e-01,  3.16741576e-01,\n",
       "         -1.90721076e-01,  1.30779167e-01,  1.69452152e-01,\n",
       "          8.93972798e-03,  1.83435452e-01],\n",
       "        [-7.57314063e-02, -2.11025318e-01, -1.07268060e-01,\n",
       "         -4.27842086e-01, -6.82489543e-02,  4.88869903e-01,\n",
       "         -2.75053911e-01,  5.43104019e-02,  2.29334749e-01,\n",
       "         -3.06974905e-01,  2.04188365e-01],\n",
       "        [-2.38099962e+00, -5.50228597e-01,  3.75500773e-01,\n",
       "         -1.00269393e+00,  4.55009673e-01, -2.82677619e-01,\n",
       "         -6.15294487e-01,  4.72645269e-01,  5.70998432e-01,\n",
       "          1.34351312e-01, -1.05623969e-01],\n",
       "        [ 1.81357826e-01,  4.11362343e-01, -8.20467859e-01,\n",
       "          6.31918802e-01, -1.09865159e+00,  5.69956607e-01,\n",
       "         -1.23811023e-01, -6.72514918e-01,  1.38129968e+00,\n",
       "          3.87271430e-01,  3.16611756e-01],\n",
       "        [-2.75171422e-01,  1.45713959e-01, -2.61374994e-01,\n",
       "         -2.76534487e-01,  9.70524424e-02, -1.91498264e-01,\n",
       "          6.67588194e-01, -4.09045510e-01,  7.66482744e-02,\n",
       "         -4.27604519e-01,  3.46247155e-01],\n",
       "        [ 8.09112147e-02, -2.52920292e-01,  4.20048484e-02,\n",
       "          3.77678245e-01, -2.33953599e-01, -3.09863016e-03,\n",
       "         -5.16967680e-01,  3.23349210e-01, -1.26754989e-01,\n",
       "          1.95844186e-01, -2.95753627e-01],\n",
       "        [ 1.11512049e-01,  9.78637979e-02,  3.98603741e-01,\n",
       "         -4.38208424e-02,  4.36082989e-01, -2.19655409e-01,\n",
       "         -2.12413570e-01,  4.95209780e-02,  2.42172000e-01,\n",
       "         -1.64515392e-01,  3.39013228e-01],\n",
       "        [ 5.42451665e-02,  5.81369939e-02,  7.60750750e-01,\n",
       "          2.51154347e-01,  2.81590323e-01, -1.07523734e-01,\n",
       "         -1.40177100e-01,  1.89244344e-01, -1.05539259e-01,\n",
       "         -5.01155931e-01,  3.31275490e-02]]),\n",
       " array([[-1.09490515,  0.64040426,  0.62544394,  0.33230682, -0.92742996,\n",
       "          0.31440969,  0.76763285,  0.6166689 , -0.00996744,  0.36948197,\n",
       "          0.79729831],\n",
       "        [ 0.52784509, -0.97856194, -0.08204964, -1.04154682, -0.23484294,\n",
       "          0.3048961 , -0.2802284 ,  0.07046569, -0.88163823,  0.19672596,\n",
       "          0.0825082 ],\n",
       "        [-0.52351277,  0.30975803, -0.76744729, -1.29559249,  0.17248549,\n",
       "         -0.73083988, -1.04768129, -0.42308215, -0.05978055, -0.27519363,\n",
       "          0.16454818],\n",
       "        [ 0.57172771,  0.20783021, -0.51708031, -1.76006409,  0.20741341,\n",
       "         -1.38523972, -0.65268447,  0.25535021, -0.1735672 , -0.36616841,\n",
       "         -0.67788885],\n",
       "        [ 0.30345497, -0.32569112,  0.47980006, -0.64897175,  0.56813159,\n",
       "          0.49866563,  0.344114  ,  0.70487868, -0.20122195, -0.1443608 ,\n",
       "         -0.66466203],\n",
       "        [ 0.27914733,  0.4274801 , -0.07490994,  0.14485847,  0.15366783,\n",
       "          0.50327799,  0.75778036, -0.61510182, -0.73790763,  0.22306099,\n",
       "         -0.31027744],\n",
       "        [-0.73610924, -0.59370713,  0.12802241, -3.01705752,  0.16079868,\n",
       "         -0.04591506, -0.13892529,  0.54907019, -0.26558858,  0.33019618,\n",
       "         -0.04219567],\n",
       "        [-0.21323708,  0.13792931, -0.29025877, -1.76690005,  0.63149128,\n",
       "          0.31017506, -0.55210443, -0.11599883, -0.2324202 ,  0.51382391,\n",
       "         -0.09277951],\n",
       "        [-0.28435645,  0.6114643 ,  0.14591484, -1.64750506, -0.92339555,\n",
       "          0.173816  , -0.02146982, -0.25660409,  0.62665508,  0.54954008,\n",
       "         -0.01621887],\n",
       "        [ 0.20922396, -0.10050709, -1.92968482, -0.88138123,  0.02490233,\n",
       "         -0.0974155 ,  0.01800709, -0.4292901 , -0.24736215,  0.12654189,\n",
       "          0.44131369],\n",
       "        [-0.23969854, -0.42089984, -0.3592745 ,  0.37493032, -0.41968405,\n",
       "         -0.6659023 ,  0.20779549, -0.63670943, -0.02291334,  0.1454858 ,\n",
       "         -0.04242892]]),\n",
       " array([[-0.93421529],\n",
       "        [ 0.78306154],\n",
       "        [ 1.06734417],\n",
       "        [ 5.4891306 ],\n",
       "        [ 0.97039331],\n",
       "        [ 0.77345139],\n",
       "        [-1.05233824],\n",
       "        [-0.9026602 ],\n",
       "        [-0.69195966],\n",
       "        [-0.55159584],\n",
       "        [-0.79179694]])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parámetros \"óptimos\" de la red con librería:\n",
    "classifier.coefs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "Estimativo del error en datos de prueba:\n",
    "$$\n",
    "P(\\epsilon)=0.35\\% \\pm 1\n",
    "$$\n",
    "\n",
    "Finalmente, se puede evidenciar que el modelo hallado tiene un promedio en el porcentaje de aciertos cercano al 65%, esto para los tres sets de datos utilizados (entrenamiento,validación y prueba). De allí, se puede concluir que el modelo no se ha sobreajustado a los datos ya que presenta porcentajes similares en los tres sets nombrados anteriormente. Se pudo comprobar, que para 3 capas cada una con 16 neuronas el modelo comenzaba a sobreajustarse presentando porcentajes de error alrededor del 84%-85% en el set de entrenamiento, mientras que en el set de validación optenía 58%-59%.\n",
    "\n",
    "Por último, se halló el error asociado a la prueba del modelo: \n",
    "\n",
    "$$\\epsilon=\\sqrt{\\frac{log(\\frac{2}{\\delta})}{2n}}$$\n",
    "$$\\epsilon=\\sqrt{\\frac{log(\\frac{2}{0.05})}{2*1650}}$$\n",
    "$$ \\epsilon=3.34\\%$$\n",
    "\n",
    "Este error representa la posibilidad de que la estimación que hagó de mi modelo sea erronea. Teniendo en cuenta que, para una confianza del 95% la probabilidad de error de la estimación es de 3.34%, se puede afirmar que la estimación del error de prueba del modelo está muy cerca de su valor real en la mayoría de los casos. \n",
    "\n",
    "Por otro lado, con el fin de probar las diferencias entre la implementación \"manual\" y la implementación que ofrece la librería ***SciKit-Learn***, se probarón varios modelos creados con el módulo ***MLPClassifier*** y se analizó las diferencias entre los parámetros encontrados por las dos implementaciones, de igual forma se observaron las diferencias en el descenso sobre la función de costo. Se evidenció, que los pesos de los dos modelos presentan rangos y valores muy cercanos, así como también lo hacen sus sesgos ó biases. Sin embargo, la implementación con librería ofrece un mejor descenso de gradiente sobre la función de costo, llegando a valores de 0.3-0.4 sin sobreajustar la red a los ejemplos de entrenamiento, mientras qué con la implementación manual, sin importar los cambios que se hicieran, no se logró obtener valores por debajo del 0.51 en la función de costo. Es posible, que la librería implemente métodos o funciones adicionales(aparte de adam) que ayuden al descenso de gradiente y eviten que la función de costo se quede estancada en mínimos espúreos. \n",
    "\n",
    "El mejor desempeño encontrado al utilizar los modelos de la librería ***SciKit-Learn*** fue del 0.69% en el porcentaje de aciertos. A pesar de que la función de costo implementada por el método \"manual\" no lográ descender a más de 0.51, la diferencia entre el desempeño del mejor modelo hallado con la librería y el mejor modelo hallado con el método manual no es muy grande, siendo tan solo del 5%-4%.\n",
    "\n",
    "En cuanto al preprocesamiento de los datos, después de probar varias veces con los 4 tipos de preprocesamiento que ofrece la función ***preprocesing***, se halló que el modelo tenía el mejor desempeño cuando se aplicaba un preprocesamiento robusto a los datos. Se presume que esto se debe a que varias columnas de la matriz de datos original eran dispersas y a que el data set original traía una cantidad de outliers significativa para el problema. Por lo cual, un tipo de preprocesamiento como el preprocesamiento robusto que no se ve afectado en gran manera por la presencia de datos ruidosos o excepcionales(outliers) es ideal para este problema. \n",
    "\n",
    "Con lo referido a PCA, se puedo observar que el mejor desempeño de la red al preprocesar los datos con PCA fue cuando se conservaban entre 25 y 28 variables lo cual hace referencia a aproximadamente el 98% de la varianza total de la base de datos. Sin embargo, el mejor resultado obtenido con PCA era del 62%-63% lo cual es peor que el hallado con la implementación robusta por un 3% en el porcentaje de aciertos.\n",
    "\n",
    "Normalizar ofrecía desempeños cercanos a los encontrados con la implementación robusta, y estandarizar no obtuvo desempeños superiores al 59% sin importar cuan complejo o simple fuese el modelo hallado.\n",
    "\n",
    "Además, se intento aplicar técnicas de secuencial hacia atrás y hacia adelante, pero ninguna de éstas logro superar el desempeño del modelo creado al entrenar con el set de datos completo(conservando todas las variables). Así mismo, se intento adicionar variables al sistema utilizando métodos como ***polinomial features*** (que consiste en agregar nuevas columnas que representen multiplicaciones entre las columnas de la matriz ó columnas de la matriz elevadas a alguna potencia) y métodos basados en ***kernels***. Sin embargo, ninguna de estas técnicas produjo méjores resultados que los obtenidos con el set de datos completo. \n",
    "\n",
    "Por último, el algoritmo de óptimización utilizado durante el entrenamiento de la red fue Adam ya que este presentaba mejores resultados tanto en tiempo de ejecución como en ajuste de los pesos de la red a comparación de los resultados obtenidos con descenso de gradiente estocástico.  Por otro lado, se probaron diferentes funciones de activación para las capas escondidas y para la capa de salida, llegando al final a la misma conclusión que en el reto 6; la mejor función de activación para las capas escondidas es la función ***ReLu*** ,y para la capa de salida la función con el mejor desempeño fue la función ***sigmoide*** o de sigmoid. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
